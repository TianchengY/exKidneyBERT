{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3941b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead,BertForSequenceClassification, AutoTokenizer,AutoModelForQuestionAnswering, AutoModel,AutoModelForMaskedLM,AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score,roc_curve\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AdamW,get_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e380c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6358f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# qa_kidneyBert.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607a4f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "new_tokens = [\"interstitial\", \"fibrosis\", \"tubular\", \"atrophy\",\"antibody\",\"T-cell\"]\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d62500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_datasets(q,train_text,test_text,tokenizer=tokenizer):\n",
    "    train_q = [q for i in range(len(train_text))]\n",
    "    test_q = [q for i in range(len(test_text))]\n",
    "\n",
    "    train_encodings = tokenizer(train_q,train_text,padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512,return_offsets_mapping=True)\n",
    "    test_encodings = tokenizer(test_q,test_text,padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512,return_offsets_mapping=True)\n",
    "    train_dataset = RenalDataset(train_encodings, train_labels)\n",
    "    test_dataset = RenalDataset(test_encodings, test_labels)\n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18577ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels,task_name=None):\n",
    "        self.encodings = encodings\n",
    "        self.answers = labels\n",
    "        self.task_name = task_name\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        answer = self.answers[idx]\n",
    "        offsets = inputs.pop(\"offset_mapping\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        cls_index = list(input_ids).index(tokenizer.cls_token_id)\n",
    "\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "#         print(\"Asd\",answer)\n",
    "\n",
    "        if answer[1] == 0:\n",
    "            inputs[\"start_positions\"] = cls_index\n",
    "            inputs[\"end_positions\"] = cls_index\n",
    "        else:\n",
    "            start_char = answer[0]\n",
    "            end_char = answer[1]\n",
    "\n",
    "            token_start_index = 0\n",
    "            while token_type_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while offsets[token_end_index][1] == 0:\n",
    "                token_end_index -= 1\n",
    "                \n",
    "#             print(offsets[token_start_index][0] , start_char,answer)\n",
    "\n",
    "#             print(token_start_index,token_end_index)\n",
    "#             print(offsets[token_start_index], offsets[token_end_index])\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                inputs[\"start_positions\"] = cls_index\n",
    "                inputs[\"end_positions\"] = cls_index\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "#                     print(offsets[token_start_index],token_start_index)\n",
    "                    token_start_index += 1\n",
    "                inputs[\"start_positions\"] = token_start_index - 1\n",
    "\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                inputs[\"end_positions\"] = token_end_index + 1\n",
    "        inputs[\"start_positions\"] = torch.tensor(inputs[\"start_positions\"])\n",
    "        inputs[\"end_positions\"] = torch.tensor(inputs[\"end_positions\"])\n",
    "#         inputs[\"labels\"] = (inputs[\"start_positions\"],inputs[\"end_positions\"])\n",
    "#         print(inputs[\"start_positions\"],inputs[\"end_positions\"])\n",
    "        return inputs\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answers)\n",
    "    \n",
    "\n",
    "def compute_metrics(p):   \n",
    "    \n",
    "    pred, labels = p   \n",
    "        \n",
    "    answer_start_scores, answer_end_scores = pred\n",
    "    answer_start = np.argmax(answer_start_scores, axis=1)  # get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = np.argmax(answer_end_scores, axis=1)+1\n",
    "    \n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for s,e,t,id in zip(answer_start,answer_end,test_ans,test_ids):\n",
    "        total += 1\n",
    "        pred_ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(id[s:e]))\n",
    "#         print(\"qweretw\",pred_ans,t,s,e)\n",
    "        if s == 0 and e == 1 and t == \"\":\n",
    "            correct += 1\n",
    "        elif pred_ans.lower()==t.lower():\n",
    "            correct += 1\n",
    "#         else:\n",
    "#             print(pred_ans.lower(),t.lower())\n",
    "    \n",
    "    return {\"accuracy\": correct/total}\n",
    "    \n",
    "#     accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "#     recall = recall_score(y_true=labels, y_pred=pred,average=\"micro\")\n",
    "#     precision = precision_score(y_true=labels, y_pred=pred,average=\"micro\")\n",
    "#     f1 = f1_score(y_true=labels, y_pred=pred,average=\"micro\")\n",
    "#     print(\"accuracy: {}, precision: {}, recall: {}, f1: {}\".format(accuracy,precision,recall,f1))\n",
    "#     return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fcb16",
   "metadata": {},
   "source": [
    "## Load ABMR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1c861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "inputs1 = data[\"train_report_qa\"].tolist()\n",
    "label1 = data[\"abmr_pos_qa\"].tolist()\n",
    "label = [eval(l) for i,l in zip(inputs1,label1) if str(i)!=\"nan\"]\n",
    "inputs = [i for i in inputs1 if str(i)!=\"nan\"]\n",
    "\n",
    "label_class_help = data[\"abmr_class\"].tolist()\n",
    "label_class = [l for i,l in zip(inputs1,label_class_help) if str(i)!=\"nan\"]\n",
    "\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(\n",
    "    inputs, label,random_state = 1,stratify=label_class,test_size=0.2)\n",
    "\n",
    "\n",
    "q_abmr = \"How is the antibody-mediated rejection?\"\n",
    "train_dataset,test_dataset = gen_datasets(q_abmr,train_text,test_text)\n",
    "abmr_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "abmr_test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = batch_size)\n",
    "\n",
    "test_ans = []\n",
    "for i,l in zip(test_text,test_labels):\n",
    "    test_ans.append(i[l[0]:l[1]])\n",
    "\n",
    "test_ids = torch.tensor([])\n",
    "for i in abmr_test_loader:\n",
    "    test_ids = torch.cat((test_ids,i[\"input_ids\"]),0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fec18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def get_overlap_ratio(s1, s2):\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    pos_a, pos_b, size = s.find_longest_match(0, len(s1), 0, len(s2)) \n",
    "#     print(s1,s2,s1[pos_a:pos_a+size])\n",
    "    return size/len(s2),len(s1[pos_a:pos_a+size].split())/len(s2.split())\n",
    "\n",
    "def compute_metrics(p):   \n",
    "    \n",
    "    pred, labels = p   \n",
    "        \n",
    "    answer_start_scores, answer_end_scores = pred\n",
    "    answer_start = np.argmax(answer_start_scores, axis=1)  # get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = np.argmax(answer_end_scores, axis=1)+1\n",
    "    \n",
    "    \n",
    "    total = 0\n",
    "    correct,correct_with_info = 0,0\n",
    "    overlap_ratio_char = []\n",
    "    overlap_ratio_word = []\n",
    "    for s,e,t,id in zip(answer_start,answer_end,test_ans,test_ids):\n",
    "        total += 1\n",
    "        pred_ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(id[s:e]))\n",
    "#         print(\"qweretw\",pred_ans,t,s,e)\n",
    "        if s == 0 and e == 1 and t == \"\":\n",
    "            correct += 1\n",
    "        elif not (s == 0 and e == 1) and t!=\"\":\n",
    "            if pred_ans.lower().replace('\\n', ' ')==t.lower():\n",
    "                correct_with_info += 1\n",
    "            char_ratio,word_ratio = get_overlap_ratio(pred_ans.lower().replace('\\n', ' '),t.lower())\n",
    "            overlap_ratio_char.append(char_ratio)\n",
    "            overlap_ratio_word.append(word_ratio)\n",
    "    \n",
    "    return {\"accuracy\": (correct+correct_with_info)/total,\"accuracy_info\": correct_with_info/total,\\\n",
    "            \"overlap_ratio_char\":np.mean(overlap_ratio_char),\"overlap_ratio_word\":np.mean(overlap_ratio_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b1f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_abmr_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./qaabmr_fine_mlm_largeData_pos/checkpoint-600\")\n",
    "# qa_abmr_kidneyBert_trainer = Trainer(qa_abmr_kidneyBert) \n",
    "# raw_pred,_,_=qa_abmr_kidneyBert_trainer.predict(test_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74354fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 543/3435 05:09 < 27:35, 1.75 it/s, Epoch 2.37/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.702400</td>\n",
       "      <td>0.079623</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.049821</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.036073</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112437</td>\n",
       "      <td>0.100682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.018535</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295811</td>\n",
       "      <td>0.377348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366086</td>\n",
       "      <td>0.480758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.017407</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.021029</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.019221</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.518578</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.022519</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-50\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-50\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-50\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-100\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-1150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-150\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-150\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-150\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-200\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-250\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-250\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-300\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-350\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-350\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-350\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-400\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-450\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-450\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-450\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_biobert_pos\\checkpoint-500\n",
      "Configuration saved in ./qaabmr_fine_biobert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaabmr_fine_biobert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_biobert_pos\\checkpoint-450] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d62c4b4bda6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_bioBert = AutoModelForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "steps = 50\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaabmr_fine_biobert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 1,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_bioBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f2eaaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./mlm_results_largeData_extended_tokenizer/checkpoint-1100 were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./mlm_results_largeData_extended_tokenizer/checkpoint-1100 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='755' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 755/3435 07:29 < 26:40, 1.67 it/s, Epoch 3.29/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.730400</td>\n",
       "      <td>0.095306</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.075608</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.051657</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.025861</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366086</td>\n",
       "      <td>0.480758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.042600</td>\n",
       "      <td>0.025303</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.017794</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332753</td>\n",
       "      <td>0.447424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.014829</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332753</td>\n",
       "      <td>0.447424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.014730</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229154</td>\n",
       "      <td>0.281591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.010389</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.017335</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263811</td>\n",
       "      <td>0.317348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.017163</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366086</td>\n",
       "      <td>0.480758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352654</td>\n",
       "      <td>0.468258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366086</td>\n",
       "      <td>0.480758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352654</td>\n",
       "      <td>0.468258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-50\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-50\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-50\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-700] due to args.save_total_limit\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-100\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-1550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-150\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-150\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-150\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-200\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-250\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-250\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-300\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-350\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-350\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-350\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-400\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-450\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-450\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-450\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-500\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-550\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-550\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-550\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-600\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-650\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-650\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-650\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-700\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_exkidbert_pos\\checkpoint-750\n",
      "Configuration saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-750\\config.json\n",
      "Model weights saved in ./qaabmr_fine_exkidbert_pos\\checkpoint-750\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_exkidbert_pos\\checkpoint-650] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-96197f626a29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_exkidBert = AutoModelForQuestionAnswering.from_pretrained(\"./mlm_results_largeData_extended_tokenizer/checkpoint-1100\")\n",
    "\n",
    "steps = 50\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaabmr_fine_exkidbert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 1,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_exkidBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c13922f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1322' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1322/3435 11:02 < 17:39, 1.99 it/s, Epoch 5.77/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.413700</td>\n",
       "      <td>0.076403</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.069766</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.033845</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.031906</td>\n",
       "      <td>0.963504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324936</td>\n",
       "      <td>0.395139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.026727</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442083</td>\n",
       "      <td>0.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.011577</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283221</td>\n",
       "      <td>0.375758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366086</td>\n",
       "      <td>0.480758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.016651</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387608</td>\n",
       "      <td>0.500947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366086</td>\n",
       "      <td>0.480758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352654</td>\n",
       "      <td>0.468258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-100\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-200\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-300\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-400\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-500\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-600\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-700\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-800\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-900\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-900\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-1000\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-1100\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-1200\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_vanbert_pos\\checkpoint-1300\n",
      "Configuration saved in ./qaabmr_fine_vanbert_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qaabmr_fine_vanbert_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaabmr_fine_vanbert_pos\\checkpoint-1100] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-510963f533e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_vanBert = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaabmr_fine_vanbert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 3,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_vanBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f79b5de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='605' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 605/3435 04:54 < 23:03, 2.05 it/s, Epoch 2.64/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>0.037542</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.019869</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242586</td>\n",
       "      <td>0.294091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011778</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.013360</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.018384</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-100\n",
      "Configuration saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-200\n",
      "Configuration saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-300\n",
      "Configuration saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-400\n",
      "Configuration saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-500\n",
      "Configuration saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-600\n",
      "Configuration saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaabmr_fine_bioasq_kidneyBert\\checkpoint-600\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-aead8fd8c698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_bioasq_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"qa_bioasq_fine_kidneybert/checkpoint-150\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaabmr_fine_bioasq_kidneyBert',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_bioasq_kidneyBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd6a600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1005' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1005/3435 08:15 < 20:00, 2.02 it/s, Epoch 4.38/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.599500</td>\n",
       "      <td>0.067645</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.026709</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.029099</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195770</td>\n",
       "      <td>0.234015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.019804</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.018984</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.020145</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.016772</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.018907</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.012994</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.015775</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-100\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-200\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-300\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-400\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-500\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-600\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-700\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-800\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-900\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_clinicalbert_pos\\checkpoint-1000\n",
      "Configuration saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qaabmr_fine_clinicalbert_pos\\checkpoint-1000\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c803b89c3145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_cinicalBert = AutoModelForQuestionAnswering.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaabmr_fine_clinicalbert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_cinicalBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d9153a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./mlm_results_largeData/checkpoint-1100 were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./mlm_results_largeData/checkpoint-1100 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1503' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1503/3435 12:17 < 15:49, 2.03 it/s, Epoch 6.56/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.711500</td>\n",
       "      <td>0.072496</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>0.037608</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112437</td>\n",
       "      <td>0.100682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.022305</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141770</td>\n",
       "      <td>0.140682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.026483</td>\n",
       "      <td>0.964964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336554</td>\n",
       "      <td>0.435758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.021258</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242586</td>\n",
       "      <td>0.294091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.015662</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336554</td>\n",
       "      <td>0.435758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.010857</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349987</td>\n",
       "      <td>0.448258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.014431</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.014759</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349987</td>\n",
       "      <td>0.448258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.015893</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363420</td>\n",
       "      <td>0.460758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.967883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349987</td>\n",
       "      <td>0.448258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-100\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-200\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-300\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-400\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-500\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-600\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-700\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-800\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-900\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1000\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1100\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1200\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1300\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1400\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1500\n",
      "Configuration saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qaabmr_fine_mlm_largeData_pos\\checkpoint-1500\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fbabbff34e75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./mlm_results_largeData/checkpoint-1100\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaabmr_fine_mlm_largeData_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_kidneyBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1dd7e",
   "metadata": {},
   "source": [
    "## Load TCMR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c45b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "inputs1 = data[\"train_report_qa\"].tolist()\n",
    "label1 = data[\"tcmr_pos_qa\"].tolist()\n",
    "label = [eval(l) for i,l in zip(inputs1,label1) if str(i)!=\"nan\"]\n",
    "inputs = [i for i in inputs1 if str(i)!=\"nan\"]\n",
    "\n",
    "label_class_help = data[\"tcmr_class\"].tolist()\n",
    "label_class = [l for i,l in zip(inputs1,label_class_help) if str(i)!=\"nan\"]\n",
    "\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(\n",
    "    inputs, label,random_state = 1,stratify=label_class,test_size=0.2)\n",
    "\n",
    "\n",
    "q_tcmr = \"How is the t-cell-mediated rejection?\"\n",
    "train_dataset,test_dataset = gen_datasets(q_tcmr,train_text,test_text)\n",
    "tcmr_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "tcmr_test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = batch_size)\n",
    "\n",
    "test_ans = []\n",
    "for i,l in zip(test_text,test_labels):\n",
    "    test_ans.append(i[l[0]:l[1]])\n",
    "\n",
    "test_ids = torch.tensor([])\n",
    "for i in tcmr_test_loader:\n",
    "    test_ids = torch.cat((test_ids,i[\"input_ids\"]),0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b265dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def get_overlap_ratio(s1, s2):\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    pos_a, pos_b, size = s.find_longest_match(0, len(s1), 0, len(s2)) \n",
    "#     print(s1,s2,s1[pos_a:pos_a+size])\n",
    "    return size/len(s2),len(s1[pos_a:pos_a+size].split())/len(s2.split())\n",
    "\n",
    "def compute_metrics(p):   \n",
    "    \n",
    "    pred, labels = p   \n",
    "        \n",
    "    answer_start_scores, answer_end_scores = pred\n",
    "    answer_start = np.argmax(answer_start_scores, axis=1)  # get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = np.argmax(answer_end_scores, axis=1)+1\n",
    "    \n",
    "    \n",
    "    total = 0\n",
    "    correct,correct_with_info = 0,0\n",
    "    overlap_ratio_char = []\n",
    "    overlap_ratio_word = []\n",
    "    for s,e,t,id in zip(answer_start,answer_end,test_ans,test_ids):\n",
    "        total += 1\n",
    "        pred_ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(id[s:e]))\n",
    "#         print(\"qweretw\",pred_ans,t,s,e)\n",
    "        if s == 0 and e == 1 and t == \"\":\n",
    "            correct += 1\n",
    "        elif not (s == 0 and e == 1) and t!=\"\":\n",
    "            if pred_ans.lower().replace('\\n', ' ')==t.lower():\n",
    "                correct_with_info += 1\n",
    "            char_ratio,word_ratio = get_overlap_ratio(pred_ans.lower().replace('\\n', ' '),t.lower())\n",
    "            overlap_ratio_char.append(char_ratio)\n",
    "            overlap_ratio_word.append(word_ratio)\n",
    "    \n",
    "    return {\"accuracy\": (correct+correct_with_info)/total,\"accuracy_info\": correct_with_info/total,\\\n",
    "            \"overlap_ratio_char\":np.mean(overlap_ratio_char),\"overlap_ratio_word\":np.mean(overlap_ratio_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5790dd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1277' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1277/3435 12:20 < 20:53, 1.72 it/s, Epoch 5.57/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.588000</td>\n",
       "      <td>0.079879</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.059720</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.043415</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.024153</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400175</td>\n",
       "      <td>0.430556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176773</td>\n",
       "      <td>0.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.015051</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259824</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.009115</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.009178</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.010403</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.010312</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.010629</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.012913</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.009131</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430051</td>\n",
       "      <td>0.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.008649</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301396</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008741</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.012064</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301396</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-50\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-50\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-50\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-100\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-150\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-150\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-150\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-200\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-250\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-250\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-300\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-350\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-350\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-350\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-400\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-450\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-450\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-450\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-500\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-550\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-550\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-550\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-600\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-650\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-650\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-650\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-700\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-750\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-750\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-750\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-800\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-850\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-850\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-850\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-900\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-900\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-900\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-950\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-950\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-950\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-850] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-1000\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-950] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-1050\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-1050\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-1050\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-1100\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-1050] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-1150\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-1150\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-1150\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-1200\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-1150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_biobert_pos\\checkpoint-1250\n",
      "Configuration saved in ./qatcmr_fine_biobert_pos\\checkpoint-1250\\config.json\n",
      "Model weights saved in ./qatcmr_fine_biobert_pos\\checkpoint-1250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_biobert_pos\\checkpoint-1200] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-764b29fa08a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_bioBert = AutoModelForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "steps = 50\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qatcmr_fine_biobert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 1,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_bioBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461c5fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./mlm_results_largeData_extended_tokenizer/checkpoint-1100 were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./mlm_results_largeData_extended_tokenizer/checkpoint-1100 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1919' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1919/3435 18:40 < 14:45, 1.71 it/s, Epoch 8.38/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.626100</td>\n",
       "      <td>0.080515</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.061449</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.058689</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.045252</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.045229</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.043424</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.040156</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.044134</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.022840</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.047014</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.040449</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.044043</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.024379</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.020254</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.048254</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.064712</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.053539</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.039791</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.014088</td>\n",
       "      <td>0.976642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664788</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.026082</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.016306</td>\n",
       "      <td>0.976642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664788</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.015658</td>\n",
       "      <td>0.976642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664788</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.015036</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663849</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.016094</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663849</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.016566</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663849</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.015343</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534353</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663849</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.014246</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664788</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.013392</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664788</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.018251</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.013328</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664788</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.013187</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663849</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.014787</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663849</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.014671</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663849</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-50\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-50\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-50\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-100\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-150\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-150\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-150\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-200\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-250\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-250\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-300\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-350\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-350\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-350\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-400\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-450\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-450\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-450\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-500\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-550\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-550\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-550\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-600\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-650\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-650\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-650\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-700\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-750\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-750\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-750\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-800\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-850\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-850\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-850\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-900\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-900\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-950\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-950\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-950\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-850] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1000\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1050\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1050\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1050\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1100\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-950] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1150\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1150\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1150\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1200\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1250\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1250\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1300\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1350\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1350\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1350\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1400\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1450\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1450\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1450\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1500\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1550\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1550\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1550\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1050] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1600\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1600\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1650\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1650\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1650\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1700\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1700\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1750\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1750\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1750\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1800\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1800\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1800\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1850\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1850\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1850\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_exkidbert_pos\\checkpoint-1900\n",
      "Configuration saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1900\\config.json\n",
      "Model weights saved in ./qatcmr_fine_exkidbert_pos\\checkpoint-1900\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_exkidbert_pos\\checkpoint-1750] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0bd1a2256ef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_exkidBert = AutoModelForQuestionAnswering.from_pretrained(\"./mlm_results_largeData_extended_tokenizer/checkpoint-1100\")\n",
    "\n",
    "steps = 50\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qatcmr_fine_exkidbert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 3,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_exkidBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbbd116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1793' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1793/3435 14:32 < 13:20, 2.05 it/s, Epoch 7.83/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.522700</td>\n",
       "      <td>0.065988</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.058312</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.044044</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.056352</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.034096</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.020429</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.016256</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.030646</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212052</td>\n",
       "      <td>0.319444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.015712</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400175</td>\n",
       "      <td>0.430556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.013070</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.011642</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.011209</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-100\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-200\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-300\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-400\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-500\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-600\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-700\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-800\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-900\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-900\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-1000\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-1100\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-1200\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-1300\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-1400\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-1500\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-1600\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1600\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_vanlbert_pos\\checkpoint-1700\n",
      "Configuration saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1700\\config.json\n",
      "Model weights saved in ./qatcmr_fine_vanlbert_pos\\checkpoint-1700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qatcmr_fine_vanlbert_pos\\checkpoint-1500] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a46c9c634e5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_vanBert = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qatcmr_fine_vanlbert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 3,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_vanBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb15de59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/3435 08:15 < 20:04, 2.02 it/s, Epoch 4.37/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>0.036490</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430051</td>\n",
       "      <td>0.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.017078</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432113</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528604</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.015104</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-100\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-200\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-300\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-400\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-500\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-600\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-700\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-800\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-900\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-1000\n",
      "Configuration saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qatcmr_fine_bioasq_kidneyBert\\checkpoint-1000\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-05b96204d590>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_bioasq_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"qa_bioasq_fine_kidneybert/checkpoint-150\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qatcmr_fine_bioasq_kidneyBert',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_bioasq_kidneyBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a121124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1502' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1502/3435 12:19 < 15:53, 2.03 it/s, Epoch 6.55/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.638100</td>\n",
       "      <td>0.064063</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.056944</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.039585</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.028145</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.011739</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334958</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.014642</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194607</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.015392</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141975</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.013156</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194607</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.021456</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301396</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.014127</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.015452</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-100\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-200\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-300\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-400\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-500\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-600\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-700\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-800\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-900\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-1000\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-1100\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-1200\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-1300\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-1400\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_clinicalbert_pos\\checkpoint-1500\n",
      "Configuration saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_clinicalbert_pos\\checkpoint-1500\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c9352677add9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_cinicalBert = AutoModelForQuestionAnswering.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qatcmr_fine_clinicalbert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_cinicalBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4221f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./mlm_results_largeData/checkpoint-1100 were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./mlm_results_largeData/checkpoint-1100 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1507' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1507/3435 12:22 < 15:51, 2.03 it/s, Epoch 6.58/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.791900</td>\n",
       "      <td>0.061571</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.041403</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.039312</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.018546</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400175</td>\n",
       "      <td>0.430556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108187</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.013455</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494378</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334958</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334958</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.013116</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.010912</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.010640</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267607</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-100\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-200\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-300\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-400\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-500\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-600\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-700\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-800\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-900\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1000\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1100\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1200\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1300\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1400\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1500\n",
      "Configuration saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qatcmr_fine_mlm_largeData_pos\\checkpoint-1500\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-609362b232ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./mlm_results_largeData/checkpoint-1100\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qatcmr_fine_mlm_largeData_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_kidneyBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb73a6c",
   "metadata": {},
   "source": [
    "## Load Polyomavirus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a13fa81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "inputs1 = data[\"train_report_qa\"].tolist()\n",
    "label1 = data[\"ispoly_pos_qa\"].tolist()\n",
    "label = [eval(l) for i,l in zip(inputs1,label1) if str(i)!=\"nan\"]\n",
    "inputs = [i for i in inputs1 if str(i)!=\"nan\"]\n",
    "\n",
    "label_class_help = data[\"poly_infection\"].tolist()\n",
    "label_class = [l for i,l in zip(inputs1,label_class_help) if str(i)!=\"nan\"]\n",
    "\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(\n",
    "    inputs, label,random_state = 1,stratify=label_class,test_size=0.2)\n",
    "\n",
    "\n",
    "q_poly = \"Is there any polyomavirus infection?\"\n",
    "train_dataset,test_dataset = gen_datasets(q_poly,train_text,test_text)\n",
    "poly_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "poly_test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = batch_size)\n",
    "\n",
    "test_ans = []\n",
    "for i,l in zip(test_text,test_labels):\n",
    "    test_ans.append(i[l[0]:l[1]])\n",
    "\n",
    "test_ids = torch.tensor([])\n",
    "for i in poly_test_loader:\n",
    "    test_ids = torch.cat((test_ids,i[\"input_ids\"]),0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7846ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='903' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 903/3435 07:57 < 22:22, 1.89 it/s, Epoch 3.94/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.053400</td>\n",
       "      <td>0.030049</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.009902</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.006568</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.007845</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.959124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.014173</td>\n",
       "      <td>0.959124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_clinicalbert_pos\\checkpoint-100\n",
      "Configuration saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_clinicalbert_pos\\checkpoint-200\n",
      "Configuration saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_clinicalbert_pos\\checkpoint-300\n",
      "Configuration saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_clinicalbert_pos\\checkpoint-400\n",
      "Configuration saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_clinicalbert_pos\\checkpoint-500\n",
      "Configuration saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_clinicalbert_pos\\checkpoint-600\n",
      "Configuration saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_clinicalbert_pos\\checkpoint-700\n",
      "Configuration saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_clinicalbert_pos\\checkpoint-800\n",
      "Configuration saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_clinicalbert_pos\\checkpoint-900\n",
      "Configuration saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qapoly_fine_clinicalbert_pos\\checkpoint-900\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d32d63c4479d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_cinicalBert = AutoModelForQuestionAnswering.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qapoly_fine_clinicalbert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_cinicalBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04c1e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./mlm_results_largeData/checkpoint-1100 were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./mlm_results_largeData/checkpoint-1100 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1503' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1503/3435 13:15 < 17:04, 1.89 it/s, Epoch 6.56/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.021900</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.009134</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.008893</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.959124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.960584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.959124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.959124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-100\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-200\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-300\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-400\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-500\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-600\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-700\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-800\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-900\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-1000\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-1100\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-1200\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-1300\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-1400\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qapoly_fine_mlm_largeData_pos\\checkpoint-1500\n",
      "Configuration saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qapoly_fine_mlm_largeData_pos\\checkpoint-1500\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e2701f96b492>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./mlm_results_largeData/checkpoint-1100\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qapoly_fine_mlm_largeData_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_kidneyBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df73c30",
   "metadata": {},
   "source": [
    "## Load IFTA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9acdbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "inputs1 = data[\"train_report_qa\"].tolist()\n",
    "label1 = data[\"ifta_pos_qa\"].tolist()\n",
    "label = [eval(l) for i,l in zip(inputs1,label1) if str(i)!=\"nan\"]\n",
    "inputs = [i for i in inputs1 if str(i)!=\"nan\"]\n",
    "\n",
    "label_class_help1 = data[\"IFTA\"].tolist()\n",
    "label_class_help2 = [l for i,l in zip(inputs1,label_class_help1) if str(i)!=\"nan\"]\n",
    "label_class = [0 if l in [\"nosig\",\"minimal\",\"noinfo\"] else (1 if l==\"mild\" else (2 if l==\"moderate\" else 3)) for l in label_class_help2]\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(\n",
    "    inputs, label,random_state = 1,stratify=label_class,test_size=0.2)\n",
    "\n",
    "\n",
    "q_ifta = \"What is the grade of interstitial fibrosis and tubular atrophy?\"\n",
    "train_dataset,test_dataset = gen_datasets(q_ifta,train_text,test_text)\n",
    "ifta_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "ifta_test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = batch_size)\n",
    "\n",
    "\n",
    "\n",
    "test_ans = []\n",
    "for i,l in zip(test_text,test_labels):\n",
    "    test_ans.append(i[l[0]:l[1]])\n",
    "\n",
    "test_ids = torch.tensor([])\n",
    "for i in ifta_test_loader:\n",
    "    test_ids = torch.cat((test_ids,i[\"input_ids\"]),0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b9ce612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1913' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1913/3435 20:53 < 16:37, 1.53 it/s, Epoch 8.35/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.445600</td>\n",
       "      <td>1.250526</td>\n",
       "      <td>0.832117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.601400</td>\n",
       "      <td>0.215020</td>\n",
       "      <td>0.918248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.204860</td>\n",
       "      <td>0.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.199551</td>\n",
       "      <td>0.912409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.198800</td>\n",
       "      <td>0.276754</td>\n",
       "      <td>0.941606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.180952</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.240210</td>\n",
       "      <td>0.915328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.179767</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.205316</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.193516</td>\n",
       "      <td>0.951825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.186771</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.167818</td>\n",
       "      <td>0.951825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.166653</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.166582</td>\n",
       "      <td>0.941606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.163292</td>\n",
       "      <td>0.953285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.229218</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>0.211071</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.108600</td>\n",
       "      <td>0.203043</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.202645</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>0.219441</td>\n",
       "      <td>0.941606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>0.208902</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>0.189921</td>\n",
       "      <td>0.951825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.143257</td>\n",
       "      <td>0.950365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.193990</td>\n",
       "      <td>0.954745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.178122</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.170231</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.167965</td>\n",
       "      <td>0.950365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.207996</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.179278</td>\n",
       "      <td>0.943066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.208243</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>0.186109</td>\n",
       "      <td>0.956204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.235727</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.201816</td>\n",
       "      <td>0.951825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.236603</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.232334</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.214058</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.248526</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.279635</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-50\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-50\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-50\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-100\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-150\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-150\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-150\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-200\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-250\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-250\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-300\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-350\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-350\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-350\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-400\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-450\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-450\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-450\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-500\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-550\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-550\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-550\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-600\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-650\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-650\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-650\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-700\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-750\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-750\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-750\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-800\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-850\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-850\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-850\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-900\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-900\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-850] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-950\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-950\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-950\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1000\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-950] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1050\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1050\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1050\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1100\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1050] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1150\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1150\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1150\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1200\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1250\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1250\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1300\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1350\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1350\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1350\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1400\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1450\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1450\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1450\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1500\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1550\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1550\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1550\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1600\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1600\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1650\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1650\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1650\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1700\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1700\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1750\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1750\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1750\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1800\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1800\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1800\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1850\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1850\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1850\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioBert_extoken\\checkpoint-1900\n",
      "Configuration saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1900\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioBert_extoken\\checkpoint-1900\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_bioBert_extoken\\checkpoint-1850] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-57f4b32ba0ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_bioBert = AutoModelForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "steps = 50\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaifta_fine_bioBert_extoken',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 1,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_bioBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdac10f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./mlm_results_largeData_extended_tokenizer/checkpoint-1100 were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./mlm_results_largeData_extended_tokenizer/checkpoint-1100 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1832' max='1832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1832/1832 20:15, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.442800</td>\n",
       "      <td>0.775037</td>\n",
       "      <td>0.870073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>0.234367</td>\n",
       "      <td>0.927007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.245900</td>\n",
       "      <td>0.266203</td>\n",
       "      <td>0.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.200836</td>\n",
       "      <td>0.902190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.157700</td>\n",
       "      <td>0.393619</td>\n",
       "      <td>0.935766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.203311</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.187456</td>\n",
       "      <td>0.891971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.235762</td>\n",
       "      <td>0.929927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.179687</td>\n",
       "      <td>0.950365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>0.261416</td>\n",
       "      <td>0.941606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>0.226214</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.222837</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>0.950365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>0.168433</td>\n",
       "      <td>0.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.215271</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>0.200134</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.211186</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.209157</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.085700</td>\n",
       "      <td>0.259968</td>\n",
       "      <td>0.929927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>0.243835</td>\n",
       "      <td>0.940146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.270438</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.253514</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>0.187815</td>\n",
       "      <td>0.951825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.230903</td>\n",
       "      <td>0.951825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.229667</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.203467</td>\n",
       "      <td>0.953285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.194693</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.241733</td>\n",
       "      <td>0.951825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.248073</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.236827</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.257411</td>\n",
       "      <td>0.953285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>0.278532</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.238864</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.258159</td>\n",
       "      <td>0.950365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.258451</td>\n",
       "      <td>0.950365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.262417</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-50\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-50\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-50\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-100\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-150\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-150\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-150\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-200\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-250\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-250\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-300\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-350\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-350\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-350\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-400\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-450\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-450\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-450\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-500\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-550\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-550\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-550\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-600\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-650\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-650\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-650\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-700\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-750\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-750\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-750\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-800\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-850\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-850\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-850\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-900\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-900\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-850] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-950\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-950\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-950\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1000\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-950] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1050\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1050\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1050\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1100\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1050] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1150\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1150\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1150\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1200\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1250\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1250\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1250\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1300\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1350\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1350\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1350\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1400\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1450\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1450\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1450\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1500\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1500\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1550\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1550\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1550\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1600\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1600\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1600\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1650\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1650\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1650\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1700\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1700\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1700\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1750\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1750\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1750\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_kidBert_extoken\\checkpoint-1800\n",
      "Configuration saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1800\\config.json\n",
      "Model weights saved in ./qaifta_fine_kidBert_extoken\\checkpoint-1800\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_kidBert_extoken\\checkpoint-1750] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./qaifta_fine_kidBert_extoken\\checkpoint-700 (score: 0.16843251883983612).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1832, training_loss=0.23592356810403184, metrics={'train_runtime': 1217.5518, 'train_samples_per_second': 17.99, 'train_steps_per_second': 1.505, 'total_flos': 5723444159545344.0, 'train_loss': 0.23592356810403184, 'epoch': 8.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_kidBert_extoken = AutoModelForQuestionAnswering.from_pretrained(\"./mlm_results_largeData_extended_tokenizer/checkpoint-1100\")\n",
    "\n",
    "steps = 50\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaifta_fine_kidBert_extoken',          \n",
    "    num_train_epochs=8,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 1,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_kidBert_extoken,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bc219d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3435' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3435/3435 30:54, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.366800</td>\n",
       "      <td>0.360810</td>\n",
       "      <td>0.908029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.249229</td>\n",
       "      <td>0.891971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.294497</td>\n",
       "      <td>0.921168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.412637</td>\n",
       "      <td>0.916788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.236165</td>\n",
       "      <td>0.921168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>0.235109</td>\n",
       "      <td>0.925547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.203006</td>\n",
       "      <td>0.918248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.245231</td>\n",
       "      <td>0.935766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.288348</td>\n",
       "      <td>0.929927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.228515</td>\n",
       "      <td>0.929927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.224891</td>\n",
       "      <td>0.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>0.273503</td>\n",
       "      <td>0.928467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.224798</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.289510</td>\n",
       "      <td>0.935766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.941606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.290299</td>\n",
       "      <td>0.938686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.270213</td>\n",
       "      <td>0.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.267397</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.392970</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.291608</td>\n",
       "      <td>0.940146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.349846</td>\n",
       "      <td>0.932847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.470511</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.391868</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.348230</td>\n",
       "      <td>0.935766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.377541</td>\n",
       "      <td>0.935766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.355845</td>\n",
       "      <td>0.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.393052</td>\n",
       "      <td>0.935766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.422304</td>\n",
       "      <td>0.938686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>0.404237</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.438997</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.464934</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.463141</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.470834</td>\n",
       "      <td>0.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.481448</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-100\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-200\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-300\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-400\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-500\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-600\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-700\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-800\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-900\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1000\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1100\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1200\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1300\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1400\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1500\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1600\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1600\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1700\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1700\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1800\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1800\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-1900\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-1900\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-1900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2000\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2100\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2100\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2200\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2200\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2300\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2300\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2400\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2400\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2500\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2600\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2600\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2700\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2700\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2800\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2800\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-2900\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-2900\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-2900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-3000\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-3100\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-3100\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-3100\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_vanbert_pos\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-3200\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-3200\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-3200\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_vanbert_pos\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-3300\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-3300\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-3300\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_vanbert_pos\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_vanbert_pos\\checkpoint-3400\n",
      "Configuration saved in ./qaifta_fine_vanbert_pos\\checkpoint-3400\\config.json\n",
      "Model weights saved in ./qaifta_fine_vanbert_pos\\checkpoint-3400\\pytorch_model.bin\n",
      "Deleting older checkpoint [qaifta_fine_vanbert_pos\\checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./qaifta_fine_vanbert_pos\\checkpoint-700 (score: 0.20300598442554474).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3435, training_loss=0.16696682375417873, metrics={'train_runtime': 1857.3434, 'train_samples_per_second': 22.112, 'train_steps_per_second': 1.849, 'total_flos': 1.073145779914752e+16, 'train_loss': 0.16696682375417873, 'epoch': 15.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_vanBert = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaifta_fine_vanbert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_vanBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a52f5c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1608' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1608/3435 14:31 < 16:31, 1.84 it/s, Epoch 7.02/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.676200</td>\n",
       "      <td>0.208134</td>\n",
       "      <td>0.929927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>0.170830</td>\n",
       "      <td>0.924088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.155113</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.161913</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.260255</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.189022</td>\n",
       "      <td>0.950365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.174233</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.245111</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.198920</td>\n",
       "      <td>0.925547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.220292</td>\n",
       "      <td>0.940146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.242293</td>\n",
       "      <td>0.950365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.255707</td>\n",
       "      <td>0.951825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.253866</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.305547</td>\n",
       "      <td>0.943066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.270966</td>\n",
       "      <td>0.941606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.321138</td>\n",
       "      <td>0.938686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-100\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-200\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-300\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-400\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-500\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-600\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-700\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-800\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-900\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-1000\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-1100\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-1200\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-1300\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-1400\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-1500\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_bioasq_kidneybert\\checkpoint-1600\n",
      "Configuration saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1600\\config.json\n",
      "Model weights saved in ./qaifta_fine_bioasq_kidneybert\\checkpoint-1600\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7a41db7fd5cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_bioasq_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"qa_bioasq_fine_kidneybert/checkpoint-150\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaifta_fine_bioasq_kidneybert',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_bioasq_kidneyBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d3e825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1610' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1610/3435 14:20 < 16:16, 1.87 it/s, Epoch 7.03/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.429900</td>\n",
       "      <td>0.295083</td>\n",
       "      <td>0.922628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.253300</td>\n",
       "      <td>0.258619</td>\n",
       "      <td>0.883212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.202710</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>0.250626</td>\n",
       "      <td>0.935766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.202432</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.175798</td>\n",
       "      <td>0.940146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.123900</td>\n",
       "      <td>0.158054</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.119600</td>\n",
       "      <td>0.243413</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.172788</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>0.239777</td>\n",
       "      <td>0.928467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.249348</td>\n",
       "      <td>0.943066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.228999</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.229478</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.320267</td>\n",
       "      <td>0.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.271972</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>0.215885</td>\n",
       "      <td>0.944526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/58 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-100\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-200\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-300\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-400\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-500\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-600\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-700\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-800\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-900\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-1000\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-1100\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-1200\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-1300\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-1400\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-1500\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_clincalbert_pos\\checkpoint-1600\n",
      "Configuration saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1600\\config.json\n",
      "Model weights saved in ./qaifta_fine_clincalbert_pos\\checkpoint-1600\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4cf69412d100>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_cinicalBert = AutoModelForQuestionAnswering.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaifta_fine_clincalbert_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_cinicalBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9fde7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./mlm_results_largeData/checkpoint-1100 were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./mlm_results_largeData/checkpoint-1100 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2738\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1612' max='3435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1612/3435 14:22 < 16:16, 1.87 it/s, Epoch 7.03/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.458800</td>\n",
       "      <td>0.220624</td>\n",
       "      <td>0.929927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.191555</td>\n",
       "      <td>0.896350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.190833</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.141437</td>\n",
       "      <td>0.947445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.184365</td>\n",
       "      <td>0.945985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.185172</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.161291</td>\n",
       "      <td>0.943066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.231033</td>\n",
       "      <td>0.941606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.288754</td>\n",
       "      <td>0.928467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.260730</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.287743</td>\n",
       "      <td>0.941606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.269715</td>\n",
       "      <td>0.950365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>0.290169</td>\n",
       "      <td>0.934307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.290752</td>\n",
       "      <td>0.948905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>0.283999</td>\n",
       "      <td>0.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.306401</td>\n",
       "      <td>0.931387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-100\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-200\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-300\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-400\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-500\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-600\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-600\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-700\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-700\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-800\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-800\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-900\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-900\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-1000\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-1100\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-1200\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-1300\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-1400\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-1500\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 685\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./qaifta_fine_mlm_largeData_pos\\checkpoint-1600\n",
      "Configuration saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1600\\config.json\n",
      "Model weights saved in ./qaifta_fine_mlm_largeData_pos\\checkpoint-1600\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f734cecb2c3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1341\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-55be9bf69e12>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"end_positions\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[1;32mwhile\u001b[0m \u001b[0mtoken_start_index\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0moffsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_start_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mstart_char\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;31m#                     print(offsets[token_start_index],token_start_index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[0mtoken_start_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./mlm_results_largeData/checkpoint-1100\")\n",
    "\n",
    "steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qaifta_fine_mlm_largeData_pos',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_kidneyBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e0e4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_report(model):    \n",
    "    test_trainer = Trainer(model) \n",
    "    raw_pred,_,_=test_trainer.predict(test_dataset) \n",
    "    answer_start_scores, answer_end_scores = raw_pred\n",
    "    answer_start = np.argmax(answer_start_scores, axis=1)  # get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = np.argmax(answer_end_scores, axis=1)+1\n",
    "    \n",
    "    ans_list = [\"severe\", \"moderate\",\"mild\",\"minimal\",\"no significant\",\"\"]\n",
    "    label_list = list(range(6))\n",
    "    ans_dict = {ans:label for ans,label in zip(ans_list,label_list)}\n",
    "    \n",
    "    \n",
    "    not_in_label = 0\n",
    "    in_label = 0\n",
    "    pred_list,true_list = [],[]\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for s,e,t,id in zip(answer_start,answer_end,test_ans,test_ids):\n",
    "        total += 1\n",
    "        pred_ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(id[s:e]))\n",
    "#         print(\"qweretw\",pred_ans,t,s,e)\n",
    "\n",
    "        if s == 0 and e == 1:\n",
    "            pred_ans = \"\"\n",
    "            \n",
    "        if pred_ans.lower() in ans_dict:\n",
    "            in_label+=1\n",
    "            pred_list.append(pred_ans.lower())\n",
    "            true_list.append(t.lower())\n",
    "        else:\n",
    "            not_in_label+=1\n",
    "            \n",
    "        if pred_ans.lower()==t.lower():\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(pred_ans.lower(),t.lower())\n",
    "        \n",
    "\n",
    "    print(f\"Overall exact match accuracy: {correct/total}\")\n",
    "    print(f\"Number of predictions not in labels: {not_in_label}\")\n",
    "    print(confusion_matrix(true_list,pred_list))\n",
    "    print(classification_report(true_list,pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d97118d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report: \n",
      " , MICROSCOPIC DESCRIPTION \n",
      " Light Microscopy (LM):  The following LM findings are based on hematoxylin and eosin (H&E), periodic acid-Schiff (PAS), and Masson trichrome-stained sections.  The specimen submitted for light microscopic evaluation consists of cortical tissue with at least 119 glomeruli, 6 of which are globally sclerotic.  No segmentally sclerosed glomeruli are seen.  The viable glomeruli are either unremarkable or show focal mild mesangial expansion.  The peripheral capillary walls are thin and regular.   No significant glomerulitis is seen.  No crescents, proliferation of capillary cells or necrosis of capillary tufts are identified.  The tubulointerstitium shows acute tubular injury and mild interstitial fibrosis and tubular atrophy (~5%).  No significant interstitial inflammation, tubulitis, peritubular capillaritis, vasculitis or viral cytopathic changes is identified.  The arteries show moderate intimal thickening and arterioles show moderate hyalinosis\n",
      "\n",
      "Predicted grade:  mild\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the grade of interstitial fibrosis and tubular atrophy?\"\n",
    "\n",
    "context = \", MICROSCOPIC DESCRIPTION \\n Light Microscopy (LM):  The following LM findings are based on hematoxylin and eosin (H&E), periodic acid-Schiff (PAS), and Masson trichrome-stained sections.  The specimen submitted for light microscopic evaluation consists of cortical tissue with at least 119 glomeruli, 6 of which are globally sclerotic.  No segmentally sclerosed glomeruli are seen.  The viable glomeruli are either unremarkable or show focal mild mesangial expansion.  The peripheral capillary walls are thin and regular.   No significant glomerulitis is seen.  No crescents, proliferation of capillary cells or necrosis of capillary tufts are identified.  The tubulointerstitium shows acute tubular injury and mild interstitial fibrosis and tubular atrophy (~5%).  No significant interstitial inflammation, tubulitis, peritubular capillaritis, vasculitis or viral cytopathic changes is identified.  The arteries show moderate intimal thickening and arterioles show moderate hyalinosis\"\n",
    "\n",
    "inputs = tokenizer(question, context, padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512,return_offsets_mapping=True) \n",
    "offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "\n",
    "qa_kidneyBert.cpu()\n",
    "answer = qa_kidneyBert(**inputs)\n",
    "answer_start_scores, answer_end_scores = answer[\"start_logits\"], answer[\"end_logits\"]\n",
    "answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "\n",
    "pred_ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "\n",
    "print(\"Report: \\n\", context)\n",
    "print(\"\\nPredicted grade: \", pred_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a03e094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./qaifta_fine_vanbert_pos/checkpoint-1500\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./qaifta_fine_vanbert_pos/checkpoint-1500\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./qaifta_fine_vanbert_pos/checkpoint-1500\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ./qaifta_fine_vanbert_pos/checkpoint-1500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 685\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [86/86 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " moderate\n",
      " no significant\n",
      "[cls] what is the grade of interstitial fibrosis and tubular atrophy? [sep] ls : ajh edited by : 03 / 17 / 17 - 1105 16192 > < microscopic description \\ n clinical history : 36 year old male with esrd secondary to fsgs. light microscopy ( lm ) : the following lm findings are based on hematoxylin and eosin ( h & e ), periodic acid - schiff ( pas ), and masson trichrome - stained sections. the material submitted for lm contains a single wedge of renal cortical tissue. more than 30 glomeruli are present for examination, of which none are globally sclerotic. the non - sclerotic glomeruli contain open capillary loops with no significant glomerulitis. the capillary walls are of normal thickness and contours. no significant mesangial matrix accumulation or mesangial hypercellularity is identified. the tubulointerstitium shows mild acute tubular injury but is otherwise generally unremarkable with the tubules arranged in a back - to - back orientation. no significant \n",
      "severe mild\n",
      "mild severe\n",
      "no significant minimal\n",
      " no significant\n",
      "no significant minimal\n",
      "no significant \n",
      "no significant \n",
      " no significant\n",
      " no significant\n",
      " no significant\n",
      " no significant\n",
      " no significant\n",
      "moderate \n",
      "no significant \n",
      " no significant\n",
      "no significant \n",
      " no significant\n",
      " no significant\n",
      " severe\n",
      "minimal mild\n",
      "mild moderate\n",
      "minimal no significant\n",
      "severe mild\n",
      "no significant \n",
      " no significant\n",
      "no significant \n",
      " no significant\n",
      " minimal\n",
      "no significant \n",
      " moderate\n",
      " mild\n",
      "no significant \n",
      "mild moderate\n",
      "mild moderate\n",
      " minimal\n",
      "no significant \n",
      "mild \n",
      "Overall exact match accuracy: 0.9416058394160584\n",
      "Number of predictions not in labels: 1\n",
      "[[ 63   1   0   1   9   0]\n",
      " [  1 199   1   0   0   2]\n",
      " [  2   0  68   0   2   0]\n",
      " [  2   3   0  71   0   0]\n",
      " [ 12   0   1   0 209   0]\n",
      " [  1   1   0   0   0  35]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "                     0.78      0.85      0.81        74\n",
      "          mild       0.98      0.98      0.98       203\n",
      "       minimal       0.97      0.94      0.96        72\n",
      "      moderate       0.99      0.93      0.96        76\n",
      "no significant       0.95      0.94      0.95       222\n",
      "        severe       0.95      0.95      0.95        37\n",
      "\n",
      "      accuracy                           0.94       684\n",
      "     macro avg       0.93      0.93      0.93       684\n",
      "  weighted avg       0.94      0.94      0.94       684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_cinicalBert = AutoModelForQuestionAnswering.from_pretrained(\"./qaifta_fine_vanbert_pos/checkpoint-1500\")\n",
    "get_result_report(qa_cinicalBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59aefae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./qaifta_fine_clincalbert_pos/checkpoint-700\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./qaifta_fine_clincalbert_pos/checkpoint-700\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./qaifta_fine_clincalbert_pos/checkpoint-700\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ./qaifta_fine_clincalbert_pos/checkpoint-700.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 685\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [86/86 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall exact match accuracy: 0.9313868613138686\n",
      "Number of predictions not in labels: 21\n",
      "[[ 46   0   0   0  18   0]\n",
      " [  0 201   2   0   0   0]\n",
      " [  0   0  67   0   1   0]\n",
      " [  0   1   0  75   0   0]\n",
      " [  0   2   0   1 213   0]\n",
      " [  0   1   0   0   0  36]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "                     1.00      0.72      0.84        64\n",
      "          mild       0.98      0.99      0.99       203\n",
      "       minimal       0.97      0.99      0.98        68\n",
      "      moderate       0.99      0.99      0.99        76\n",
      "no significant       0.92      0.99      0.95       216\n",
      "        severe       1.00      0.97      0.99        37\n",
      "\n",
      "      accuracy                           0.96       664\n",
      "     macro avg       0.98      0.94      0.95       664\n",
      "  weighted avg       0.96      0.96      0.96       664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_cinicalBert = AutoModelForQuestionAnswering.from_pretrained(\"./qaifta_fine_clincalbert_pos/checkpoint-700\")\n",
    "get_result_report(qa_cinicalBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfc5843e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./qaifta_fine_mlm_largeData_pos/checkpoint-400\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./qaifta_fine_mlm_largeData_pos/checkpoint-400\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./qaifta_fine_mlm_largeData_pos/checkpoint-400\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ./qaifta_fine_mlm_largeData_pos/checkpoint-400.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 685\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [86/86 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall exact match accuracy: 0.9474452554744526\n",
      "Number of predictions not in labels: 3\n",
      "[[ 48   0   0   0  26   0]\n",
      " [  0 202   1   0   0   0]\n",
      " [  0   0  70   0   1   0]\n",
      " [  0   1   0  75   0   0]\n",
      " [  1   0   0   1 220   0]\n",
      " [  1   1   0   0   0  34]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "                     0.96      0.65      0.77        74\n",
      "          mild       0.99      1.00      0.99       203\n",
      "       minimal       0.99      0.99      0.99        71\n",
      "      moderate       0.99      0.99      0.99        76\n",
      "no significant       0.89      0.99      0.94       222\n",
      "        severe       1.00      0.94      0.97        36\n",
      "\n",
      "      accuracy                           0.95       682\n",
      "     macro avg       0.97      0.93      0.94       682\n",
      "  weighted avg       0.95      0.95      0.95       682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./qaifta_fine_mlm_largeData_pos/checkpoint-400\")\n",
    "get_result_report(qa_kidneyBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbebb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82031651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6e6ab5b",
   "metadata": {},
   "source": [
    "## Using the prediction results from QARej to solve isRej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "214aa796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_answer(p,ids):   \n",
    "    \n",
    "#     pred, labels = p   \n",
    "        \n",
    "    answer_start_scores, answer_end_scores = p\n",
    "    answer_start = np.argmax(answer_start_scores, axis=1)  # get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = np.argmax(answer_end_scores, axis=1)+1\n",
    "    \n",
    "    pred_ans_lst = []\n",
    "    for s,e,id in zip(answer_start,answer_end,ids):\n",
    "\n",
    "        pred_ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(id[s:e]))\n",
    "#         print(\"qweretw\",pred_ans,t,s,e)\n",
    "        if s == 0 and e == 1:\n",
    "            pred_ans_lst.append(\" \")\n",
    "        else:\n",
    "            pred_ans_lst.append(pred_ans.lower().replace('\\n', ' '))\n",
    "    return pred_ans_lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a1d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_abmr_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./qaabmr_fine_mlm_largeData_pos/checkpoint-600\")\n",
    "qa_tcmr_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./qatcmr_fine_mlm_largeData_pos/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "404c23d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./qaabmr_fine_mlm_largeData_pos/checkpoint-1400\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./qaabmr_fine_mlm_largeData_pos/checkpoint-1400\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./qaabmr_fine_mlm_largeData_pos/checkpoint-1400\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ./qaabmr_fine_mlm_largeData_pos/checkpoint-1400.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2738\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='429' max='343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [343/343 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 685\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "inputs1 = data[\"train_report_qa\"].tolist()\n",
    "label1 = data[\"ABMR\"].tolist()\n",
    "label = [l for i,l in zip(inputs1,label1) if str(i)!=\"nan\"]\n",
    "inputs = [i for i in inputs1 if str(i)!=\"nan\"]\n",
    "\n",
    "label_class_help = data[\"abmr_class\"].tolist()\n",
    "label_class = [l for i,l in zip(inputs1,label_class_help) if str(i)!=\"nan\"]\n",
    "\n",
    "\n",
    "train_text, test_text, abmr_train_labels, abmr_test_labels = train_test_split(\n",
    "    inputs, label,random_state = 1,stratify=label_class,test_size=0.2)\n",
    "\n",
    "\n",
    "q_abmr = \"How is the antibody-mediated rejection?\"\n",
    "train_dataset,test_dataset = gen_datasets(q_abmr,train_text,test_text)\n",
    "abmr_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size)\n",
    "abmr_test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = batch_size)\n",
    "\n",
    "# test_ans = []\n",
    "# for i,l in zip(test_text,test_labels):\n",
    "#     test_ans.append(i[l[0]:l[1]])\n",
    "\n",
    "test_ids = torch.tensor([])\n",
    "for i in abmr_test_loader:\n",
    "    test_ids = torch.cat((test_ids,i[\"input_ids\"]),0)\n",
    "    \n",
    "train_ids = torch.tensor([])\n",
    "for i in abmr_train_loader:\n",
    "    train_ids = torch.cat((train_ids,i[\"input_ids\"]),0)\n",
    "\n",
    "qa_abmr_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./qaabmr_fine_mlm_largeData_pos/checkpoint-1400\")\n",
    "qa_abmr_kidneyBert_trainer = Trainer(qa_abmr_kidneyBert) \n",
    "raw_pred,_,_=qa_abmr_kidneyBert_trainer.predict(train_dataset) \n",
    "abmr_train_ans = get_pred_answer(raw_pred,train_ids)\n",
    "\n",
    "raw_pred,_,_=qa_abmr_kidneyBert_trainer.predict(test_dataset) \n",
    "abmr_test_ans = get_pred_answer(raw_pred,test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea49c3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "with evidence of chronic antibody mediated rejection 1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "biopsy on 11 / 3 / 2014 showed antibody - mediated rejection 1\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(abmr_train_ans,abmr_train_labels):\n",
    "    if j == 1:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de4f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93e09761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "with multiple episodes of antibody - mediated rejection 1\n",
      "  1\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(abmr_test_ans,abmr_test_labels):\n",
    "    if j == 1:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc140a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f2405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fca4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366895b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99e830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e75be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96786465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80139fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22fc0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_datasets(train_text_0,train_text,test_text_0,test_text,tokenizer=tokenizer):\n",
    "\n",
    "    train_encodings = tokenizer(train_text_0,train_text,padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512,return_offsets_mapping=True)\n",
    "    test_encodings = tokenizer(test_text_0,test_text,padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512,return_offsets_mapping=True)\n",
    "    train_dataset = RenalDataset(train_encodings, train_labels)\n",
    "    test_dataset = RenalDataset(test_encodings, test_labels)\n",
    "    return train_dataset,test_dataset\n",
    "\n",
    "class RenalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred,average=\"micro\")\n",
    "    precision = precision_score(y_true=labels, y_pred=pred,average=\"micro\")\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred,average=\"micro\")\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378059d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91478502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb092f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c9ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2ad32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7120e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aedfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a79cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328abe4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c58f4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e55714c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if a<2:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c1ec409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mild'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the grade of interstitial fibrosis and tubular atrophy?\"\n",
    "\n",
    "context = \", MICROSCOPIC DESCRIPTION \\n Light Microscopy (LM):  The following LM findings are based on hematoxylin and eosin (H&E), periodic acid-Schiff (PAS), and Masson trichrome-stained sections.  The specimen submitted for light microscopic evaluation consists of cortical tissue with at least 119 glomeruli, 6 of which are globally sclerotic.  No segmentally sclerosed glomeruli are seen.  The viable glomeruli are either unremarkable or show focal mild mesangial expansion.  The peripheral capillary walls are thin and regular.   No significant glomerulitis is seen.  No crescents, proliferation of capillary cells or necrosis of capillary tufts are identified.  The tubulointerstitium shows acute tubular injury and mild interstitial fibrosis and tubular atrophy (~5%).  No significant interstitial inflammation, tubulitis, peritubular capillaritis, vasculitis or viral cytopathic changes is identified.  The arteries show moderate intimal thickening and arterioles show moderate hyalinosis\"\n",
    "# qa_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./mlm_results_largeData/checkpoint-1100\")\n",
    "# 1. TOKENIZE THE INPUT\n",
    "# note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for \n",
    "# exploration but you cannot feed that into a model. \n",
    "inputs = tokenizer(question, context, padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512,return_offsets_mapping=True) \n",
    "offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "# 2. OBTAIN MODEL SCORES\n",
    "# the AutoModelForQuestionAnswering class includes a span predictor on top of the model. \n",
    "# the model returns answer start and end scores for each word in the text\n",
    "qa_kidneyBert.cpu()\n",
    "answer = qa_kidneyBert(**inputs)\n",
    "answer_start_scores, answer_end_scores = answer[\"start_logits\"], answer[\"end_logits\"]\n",
    "answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "\n",
    "# 3. GET THE ANSWER SPAN\n",
    "# once we have the most likely start and end tokens, we grab all the tokens between them\n",
    "# and convert tokens back to words!\n",
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b0ce26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([711, 715])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_mapping[0][205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "057d082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mild\n"
     ]
    }
   ],
   "source": [
    "for i,z in zip(test_text,test_labels):\n",
    "    print(i[z[0]:z[1]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38c3e3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inter'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(question+\"  \"+context)[716:721]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0f8403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(207.)\n"
     ]
    }
   ],
   "source": [
    "for i in test_e:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d569ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ild '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[712:716]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c186a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(205), tensor(206))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start,answer_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04b46b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  t = offset_mapping[0]\n",
    "int((t == 101).nonzero(as_tuple=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "018a8a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t[0]).index(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5076c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cf42b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,   178,  1176,  5497, 22888,   119,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"I like eating apples.\"\n",
    "inputs = tokenizer(context, padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512,return_offsets_mapping=True) \n",
    "inputs[\"input_ids\"][0][:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0375aefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1],\n",
       "        [ 1,  5],\n",
       "        [ 6,  8],\n",
       "        [ 9, 12],\n",
       "        [13, 18],\n",
       "        [19, 21],\n",
       "        [22, 27],\n",
       "        [27, 29],\n",
       "        [29, 32],\n",
       "        [32, 34],\n",
       "        [35, 37],\n",
       "        [37, 40],\n",
       "        [40, 43],\n",
       "        [44, 47],\n",
       "        [48, 51],\n",
       "        [51, 55],\n",
       "        [56, 58],\n",
       "        [58, 61],\n",
       "        [61, 63],\n",
       "        [63, 64],\n",
       "        [ 1,  1],\n",
       "        [ 1,  2],\n",
       "        [ 3,  8],\n",
       "        [ 8, 14],\n",
       "        [15, 26],\n",
       "        [27, 28],\n",
       "        [28, 29],\n",
       "        [30, 35],\n",
       "        [36, 41],\n",
       "        [41, 44],\n",
       "        [44, 46],\n",
       "        [47, 48],\n",
       "        [48, 49],\n",
       "        [49, 50],\n",
       "        [50, 51],\n",
       "        [51, 52],\n",
       "        [54, 57],\n",
       "        [58, 67],\n",
       "        [68, 69],\n",
       "        [69, 70]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "offset_mapping[0][:40]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a527f86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-2.3183e-01, -2.3798e-01, -1.8489e-01, -2.9106e-02, -3.0932e-01,\n",
       "          1.8659e-01,  3.1977e-02,  1.8660e-01,  7.9363e-02,  2.4333e-01,\n",
       "         -1.9067e-01, -2.8963e-01, -2.1170e-02, -5.5045e-02, -3.3944e-01,\n",
       "         -7.2507e-02, -2.8349e-01, -4.4287e-01, -2.6857e-01, -4.8541e-01,\n",
       "          2.0282e-01,  1.5761e-02, -3.1923e-01, -3.7885e-01,  1.9984e-01,\n",
       "         -1.6532e-01, -1.8265e-01,  1.4609e-01, -4.4513e-01, -4.9060e-01,\n",
       "         -1.1417e-01, -3.6351e-01,  4.3571e-02,  1.9040e-03, -1.3833e-01,\n",
       "          1.6810e-01, -1.2825e-01, -2.5961e-01, -7.1122e-02, -1.3075e-01,\n",
       "         -3.4616e-02, -2.2744e-01, -4.1331e-01, -5.9271e-03, -9.2514e-02,\n",
       "          2.6987e-01, -5.0002e-02, -1.5550e-01, -5.2464e-02, -1.3158e-01,\n",
       "          2.3259e-01, -1.1532e-01, -3.8052e-01,  1.7588e-01,  9.7117e-02,\n",
       "         -1.3237e-01, -3.6330e-01,  4.7035e-02,  4.0960e-02, -2.4112e-01,\n",
       "          5.9508e-01,  3.6439e-01, -9.1572e-02, -1.7662e-01, -1.5083e-01,\n",
       "          8.4475e-02,  1.6225e-01, -2.2235e-01,  2.2573e-01,  1.7105e-03,\n",
       "          9.5938e-02, -2.5138e-01,  1.0615e-01, -5.6227e-01, -3.8414e-01,\n",
       "          3.9081e-01,  2.2427e-01,  1.0407e-01,  1.8076e-01,  5.0228e-02,\n",
       "          4.0317e-01,  1.7201e-01, -1.6597e-01, -6.6592e-02, -2.4166e-02,\n",
       "          8.7067e-02,  6.5878e-02, -1.7534e-01,  6.3074e-01,  5.0000e-01,\n",
       "          4.3549e-01,  1.3939e-01, -2.5313e-01, -5.1075e-02,  7.3654e-02,\n",
       "          2.6579e-01,  1.6012e-01,  6.8381e-01,  1.7926e-01, -1.2780e-02,\n",
       "          4.7050e-01, -2.1837e-01, -7.2288e-02, -4.5703e-02,  2.5022e-01,\n",
       "         -5.7588e-01,  7.2132e-02,  2.7147e-01,  2.4622e-02, -1.7686e-01,\n",
       "          3.1649e-01, -7.8864e-02,  3.0360e-01,  3.1004e-01,  6.2149e-02,\n",
       "         -2.4020e-01,  3.2551e-01,  4.7533e-01, -7.8853e-03,  6.1255e-01,\n",
       "          7.4502e-01,  3.3315e-01,  8.8015e-02, -3.9089e-01,  3.3345e-01,\n",
       "          7.2806e-02,  4.4797e-01, -2.7769e-01, -9.2633e-02, -4.8208e-02,\n",
       "          1.7771e-01, -2.4703e-01,  5.7031e-02,  3.2472e-01, -1.0754e-02,\n",
       "          2.8742e-01, -5.5151e-02,  2.3286e-01,  1.6975e-01, -2.1080e-01,\n",
       "          1.1114e-01,  9.6407e-03, -3.3801e-01, -7.9396e-02,  4.6500e-01,\n",
       "         -2.7168e-01, -6.2254e-01, -4.9241e-01,  3.7048e-01, -8.2525e-02,\n",
       "         -2.7120e-02,  2.1594e-01,  1.8724e-01,  3.5080e-02,  2.2012e-02,\n",
       "          3.4872e-01,  2.6503e-01,  3.0848e-01,  3.2115e-01,  2.4977e-01,\n",
       "         -3.7108e-01,  3.6319e-01,  3.5043e-01,  1.7081e-01, -3.9183e-01,\n",
       "          2.6001e-02,  1.0429e-02, -1.2710e-01, -1.7662e-01, -1.4419e-01,\n",
       "         -2.8786e-01, -5.6035e-02, -1.5899e-01, -3.4423e-01, -2.1539e-02,\n",
       "          2.7045e-01, -8.8280e-01, -1.9234e-01, -3.3518e-01, -1.2905e-01,\n",
       "          3.6695e-01, -1.0172e-01, -1.9450e-01,  2.7213e-01, -2.2259e-01,\n",
       "          1.5659e-01, -4.3217e-01,  2.9747e-01,  1.3065e-01, -2.5881e-02,\n",
       "          1.9991e-02, -2.2930e-01,  1.8932e-01,  4.7800e-01,  2.6427e-02,\n",
       "         -6.1774e-02, -2.0788e-01,  2.0240e-01, -1.4511e-01,  2.2129e-01,\n",
       "         -1.9222e-01,  2.5606e-01,  6.8840e-01,  5.3408e-02,  6.3862e-01,\n",
       "          2.5769e-01,  4.3563e-02,  3.6738e-01,  2.0692e-01, -3.8047e-01,\n",
       "         -4.0794e-02,  1.9931e-01, -1.8819e-02,  2.3805e-01, -2.2954e-01,\n",
       "         -1.2796e-01,  7.4841e-02,  7.3830e-02, -2.3769e-01,  3.7775e-03,\n",
       "         -1.4084e-01, -3.5440e-01, -2.0709e-01,  3.5096e-02, -2.4591e-01,\n",
       "         -8.4250e-02, -2.1796e-01,  4.0309e-02,  1.3291e-01,  1.8314e-01,\n",
       "         -1.4694e-01, -7.2783e-02, -2.4918e-01, -1.1902e-01,  4.2499e-02,\n",
       "          8.5187e-02,  2.8810e-01, -3.6065e-02, -3.9131e-02, -3.4124e-02,\n",
       "          3.4646e-01, -2.7179e-01, -2.4981e-01, -3.9914e-01, -3.1629e-01,\n",
       "         -2.5952e-01,  4.0034e-01,  4.5549e-01,  3.5571e-01,  3.2563e-01,\n",
       "          6.1618e-02, -6.5085e-01, -6.8191e-02, -2.0898e-02, -1.7152e-01,\n",
       "         -2.5329e-01, -2.0367e-01,  1.3521e-01,  4.7313e-01, -2.3315e-02,\n",
       "          2.8468e-01,  4.6225e-01,  1.2105e-01,  2.4379e-01, -3.3719e-02,\n",
       "         -6.9474e-02,  5.3807e-01, -6.5443e-02,  2.4620e-01, -1.5759e-01,\n",
       "          3.1488e-01,  6.4555e-01,  4.3256e-01,  2.7532e-01, -4.3993e-01,\n",
       "          1.1909e-01, -1.4010e-01,  3.4631e-03, -1.0529e-01,  3.7949e-01,\n",
       "          3.7238e-01,  6.8564e-02, -2.4221e-01, -1.3338e-01,  8.0932e-02,\n",
       "         -2.0414e-01, -4.3824e-02, -1.7696e-01,  9.1625e-02, -1.3100e-01,\n",
       "         -5.3667e-02, -2.8680e-01, -1.0189e-02,  2.8889e-01, -1.7931e-03,\n",
       "         -1.6748e-01,  4.1994e-01, -3.6931e-01, -6.2379e-02,  1.1659e-01,\n",
       "         -1.1710e-01, -6.3251e-02,  1.5050e-01,  2.7937e-01, -1.6349e-01,\n",
       "         -1.0208e-01,  3.2234e-01,  1.0260e-01, -1.1516e-01, -1.3527e-02,\n",
       "          1.4166e-01, -1.5602e-02,  3.7136e-01, -3.2087e-01, -1.7299e-01,\n",
       "          1.1994e-01,  2.1927e-01,  8.1886e-02, -1.1044e-01,  7.3699e-02,\n",
       "          1.0896e-01, -7.0718e-02, -1.3114e-02, -8.5246e-02,  3.1593e-01,\n",
       "         -1.6523e-01,  6.3966e-01, -8.1774e-02,  3.7659e-01, -5.4648e-02,\n",
       "         -7.3870e-02, -5.8159e-02,  2.3870e-01, -6.9476e-02, -2.8153e-01,\n",
       "          3.8743e-01, -5.9055e-04, -4.3724e-02,  7.7620e-02,  2.2417e-01,\n",
       "         -2.5462e-01,  3.3060e-01,  4.1650e-02, -3.2660e-01, -4.6831e-01,\n",
       "          1.0999e-01,  6.2596e-02,  4.3547e-01,  3.5953e-01,  2.9047e-01,\n",
       "          5.6971e-02,  4.6090e-01,  2.9135e-01,  4.7959e-02,  2.8248e-01,\n",
       "          3.5125e-02,  2.9806e-02,  2.1418e-01,  1.4994e-01,  6.8608e-02,\n",
       "         -6.8844e-02,  5.0824e-01,  1.3501e-01, -1.7733e-02,  3.7262e-01,\n",
       "          4.5557e-01, -2.8293e-01,  5.1429e-01, -2.0744e-01, -2.5394e-01,\n",
       "         -4.0029e-01, -8.9049e-02,  2.8270e-01, -7.0383e-04,  4.0231e-01,\n",
       "          1.2694e-01,  5.9274e-01,  2.9619e-01,  1.5134e-01, -1.3886e-02,\n",
       "          1.8936e-01,  5.8563e-01, -3.1622e-02,  9.0049e-02, -1.2922e-01,\n",
       "          9.7652e-02,  3.0092e-01,  6.7551e-02,  3.3183e-01,  1.6118e-01,\n",
       "          3.3330e-01,  1.5792e-01, -5.3574e-02,  8.2906e-01,  2.2759e-01,\n",
       "         -1.2983e-01, -6.5763e-02,  3.5884e-01,  1.0733e-02,  2.9183e-01,\n",
       "         -5.9070e-02,  3.0041e-01, -1.6329e-02,  2.5678e-01,  5.7731e-01,\n",
       "         -2.2898e-01,  3.7327e-01, -1.4112e-01,  7.0224e-02,  2.0738e-01,\n",
       "          1.8028e-02,  1.0309e-01, -1.5986e-01,  1.1275e-01,  2.8847e-01,\n",
       "         -2.3423e-02,  4.6428e-01,  6.2742e-01,  1.0728e-01, -8.5995e-03,\n",
       "         -3.1630e-01,  2.2653e-01,  2.3628e-01, -2.4596e-01,  2.7457e-01,\n",
       "         -1.3646e-01, -1.0563e-01,  3.1086e-01,  2.2652e-01, -1.4999e-01,\n",
       "         -1.4412e-01, -2.1120e-01, -1.5834e-01, -8.0188e-02,  3.0303e-01,\n",
       "          1.8269e-01, -1.9149e-01,  2.1374e-01, -1.7616e-01, -8.3883e-02,\n",
       "          2.8535e-01,  1.7286e-01, -1.9851e-01, -1.1345e-01, -1.9879e-01,\n",
       "         -1.0694e-01,  3.1119e-01,  1.6346e-01,  2.0396e-02, -2.0972e-01,\n",
       "         -2.7425e-01, -1.3952e-01, -7.8431e-02,  3.8048e-01,  1.1457e-01,\n",
       "         -1.9567e-01, -4.0028e-02,  3.3082e-01,  2.4097e-01, -1.0466e-01,\n",
       "         -2.0271e-02,  3.9537e-01,  1.4961e-01, -3.1754e-01,  1.7420e-01,\n",
       "         -1.1116e-01, -8.0768e-02,  2.4339e-01, -2.0185e-01, -3.5159e-01,\n",
       "         -3.3708e-01,  2.6112e-03, -3.9321e-01, -2.1805e-01, -6.5553e-02,\n",
       "          1.4614e-02, -3.8670e-01,  3.1576e-01,  1.6748e-01,  1.8968e-01,\n",
       "          1.5226e-01,  1.0831e-01,  4.9099e-02,  2.9695e-01,  1.7351e-01,\n",
       "         -1.1295e-01, -8.4066e-02, -7.3254e-02,  1.4721e-01, -6.1126e-02,\n",
       "         -1.8193e-01, -1.4579e-01,  2.6075e-01,  3.6042e-01,  3.7475e-01,\n",
       "          2.4626e-01,  2.0282e-01, -1.3971e-01,  9.2506e-02,  4.2858e-02,\n",
       "          2.3013e-02,  2.7127e-02,  1.3370e-01,  1.7408e-02, -6.6129e-02,\n",
       "         -8.4357e-03, -7.2821e-03, -1.3469e-01, -1.6551e-01, -3.8942e-02,\n",
       "         -3.7754e-02,  1.0914e-01]], grad_fn=<CloneBackward0>), end_logits=tensor([[ 0.0670,  0.2084,  0.2179,  0.5178, -0.3902,  0.0698, -0.4274,  0.0771,\n",
       "         -0.1028,  0.1851, -0.2779,  0.1880, -0.2222,  0.5621,  0.0014,  0.2629,\n",
       "          0.2677, -0.1114, -0.0844, -0.0697,  0.4626, -0.4453,  0.1252, -0.0354,\n",
       "         -0.0506, -0.3248, -0.1384, -0.0937, -0.0911, -0.0142, -0.0952,  0.3950,\n",
       "          0.0756, -0.1387, -0.3738, -0.1881,  0.5165, -0.2794,  0.0321, -0.2104,\n",
       "         -0.0472,  0.4276, -0.1743,  0.1187, -0.3819,  0.1854, -0.1161,  0.0348,\n",
       "          0.3873, -0.0630,  0.2356,  0.1509,  0.4166, -0.5139,  0.0069, -0.0730,\n",
       "         -0.4199, -0.1846,  0.0715, -0.1876,  0.2305,  0.6831, -0.0764, -0.1265,\n",
       "          0.3185, -0.1301,  0.0862, -0.3445, -0.3330,  0.4305,  0.0040,  0.1753,\n",
       "          0.1210, -0.1112,  0.2243,  0.3279,  0.2046,  0.0766, -0.1807,  0.7154,\n",
       "         -0.0526, -0.0039, -0.0075,  0.1483, -0.2308,  0.2327,  0.2152, -0.4811,\n",
       "          0.5615,  0.2946,  0.0605,  0.5283,  0.1390, -0.2131, -0.2349, -0.1441,\n",
       "          0.1580,  0.4498,  0.7516, -0.0640, -0.0465,  0.0142,  0.2755,  0.5450,\n",
       "         -0.0583, -0.1761,  0.4185, -0.2150, -0.2992,  0.2195,  0.2733,  0.5034,\n",
       "         -0.2038,  0.7170, -0.5310,  0.1143,  0.3760, -0.0681,  0.6087, -0.0945,\n",
       "          0.2663,  0.9547, -0.4652,  0.4372,  0.5541,  0.1553,  0.0898,  0.0711,\n",
       "          0.5791,  0.0943, -0.4211,  0.6290,  0.5054,  0.7035,  0.3401,  0.3714,\n",
       "          0.0694,  0.0015,  0.7432,  0.6700,  0.3523, -0.2600,  0.4035,  0.0120,\n",
       "          0.1621,  0.0599,  0.5005, -0.3992, -0.0426,  0.7418, -0.2738, -0.1357,\n",
       "          0.5141,  0.2378,  0.3419, -0.1062,  0.2248, -0.2119,  0.7501, -0.1177,\n",
       "          0.0773,  0.8702, -0.1286, -0.2664,  0.2305,  0.2528, -0.3963,  0.3589,\n",
       "         -0.0457, -0.1315,  0.5094,  0.2083, -0.5114,  0.2791, -0.3102, -0.0913,\n",
       "         -0.0161,  0.4083,  0.9457, -0.2928, -0.1411,  0.5983, -0.2394,  0.2878,\n",
       "         -0.3962,  0.5758,  0.3099, -0.1428,  0.4488,  0.0639, -0.4038, -0.4094,\n",
       "          0.0550,  0.1520,  0.4263,  0.6593, -0.2461, -0.1047,  0.0938,  0.8602,\n",
       "          0.5405,  0.2175,  0.2817,  0.5455,  0.2606,  0.5751, -0.0963, -0.1348,\n",
       "         -0.1232,  0.3098, -0.4590,  0.0206,  0.0520,  0.0915, -0.2053,  0.1261,\n",
       "         -0.2141, -0.3234,  0.0691,  0.1634,  0.2813,  0.0174,  0.1280, -0.3809,\n",
       "          0.1626,  0.1409,  0.0187, -0.3601, -0.2743,  0.1345,  0.1704,  0.3540,\n",
       "         -0.2261, -0.0248, -0.3007, -0.1869,  0.2071,  0.1238,  0.3071,  0.0798,\n",
       "         -0.4140,  0.6741,  0.3929,  0.4979, -0.2671,  0.7858, -0.0803, -0.1718,\n",
       "         -0.0785,  0.0572,  0.0197,  0.2960,  0.4401,  0.2140,  0.6730,  0.4926,\n",
       "          0.8165, -0.0505, -0.0972,  0.6802, -0.0959,  0.0725,  0.5871, -0.0831,\n",
       "          0.5135,  0.4061,  0.5788,  0.4245, -0.3209, -0.3942,  0.1985,  0.2896,\n",
       "         -0.0903,  0.1134,  0.1589,  0.1645,  0.3886, -0.0037,  0.8660, -0.0324,\n",
       "         -0.1344, -0.1346,  0.2892, -0.1302,  0.6021,  0.0693,  0.5905,  0.0489,\n",
       "          0.0766,  0.1826, -0.0172,  0.8485, -0.2572, -0.1829, -0.2019, -0.2217,\n",
       "          0.1397, -0.4562,  0.1160,  0.1402,  0.5457, -0.0418, -0.5741, -0.3664,\n",
       "         -0.2754, -0.1141,  0.5259,  0.0196, -0.0687, -0.5552, -0.5089, -0.0828,\n",
       "          0.3874,  0.2543, -0.0093,  0.6593,  0.5358,  0.6130,  0.0275,  0.1389,\n",
       "         -0.0194,  0.0526,  0.4011,  0.0299, -0.1089,  0.5840,  0.4692, -0.7495,\n",
       "          0.1533, -0.3788, -0.2261,  0.0847, -0.3082, -0.2454, -0.0988,  0.1876,\n",
       "         -0.1910,  0.0404,  0.5636,  0.0639,  0.6594,  0.1241, -0.2688, -0.0823,\n",
       "         -0.0057,  0.1267,  0.1450,  0.1819,  0.4055,  0.0073,  0.8383,  0.0507,\n",
       "          0.0241,  0.2297,  0.1588,  0.3557,  0.3385, -0.0892,  0.1361, -0.1140,\n",
       "          0.6696,  0.0342,  0.7550,  0.0616,  0.4707,  0.1944,  0.0385,  0.0457,\n",
       "          0.2162,  0.5075,  0.0844,  0.0983, -0.1998,  0.7320,  0.1733, -0.0221,\n",
       "          0.3560,  0.6938,  0.6264,  0.1723,  0.2434,  0.8094,  0.0455,  0.4152,\n",
       "          0.3988,  0.5129,  0.5111, -0.0068,  0.1290,  0.0752,  0.1093,  0.0197,\n",
       "         -0.2054, -0.1355, -0.2889, -0.2056,  0.3834,  0.4882,  0.0878,  0.6385,\n",
       "          0.1796,  0.5634,  0.2080,  0.1044,  0.8165, -0.1979, -0.1230,  0.1464,\n",
       "          0.4705,  0.6711,  0.3668, -0.0150,  0.6002, -0.2403,  0.0163,  0.5149,\n",
       "          0.4334, -0.1343,  0.6736, -0.0098, -0.1732, -0.2563, -0.1110,  0.1414,\n",
       "         -0.5320, -0.2915,  0.2132, -0.3177, -0.0713,  0.1793, -0.2586, -0.0972,\n",
       "         -0.3527,  0.1768, -0.3453, -0.1194,  0.2101, -0.3983, -0.2545,  0.2356,\n",
       "         -0.3063, -0.1876, -0.1948, -0.2157, -0.2605,  0.2148, -0.3162, -0.2253,\n",
       "         -0.1679,  0.5036, -0.2930, -0.3204,  0.1237, -0.4156, -0.1802,  0.0493,\n",
       "          0.5698,  0.1335, -0.1625, -0.1515,  0.1909, -0.3918, -0.1982, -0.0715,\n",
       "          0.0044, -0.1968,  0.1646, -0.3474, -0.2157,  0.0563,  0.0277, -0.0853,\n",
       "          0.3850, -0.2142, -0.3176, -0.2148,  0.1356, -0.1766,  0.7146,  0.2352,\n",
       "         -0.2215,  0.6202,  0.1100, -0.0437, -0.3431,  0.3076,  0.0847, -0.0610,\n",
       "          0.1812, -0.4569, -0.1289,  0.1593,  0.2365,  0.5070, -0.5363, -0.0631,\n",
       "          0.4626, -0.2167, -0.1083,  0.0364,  0.0492, -0.0133,  0.0084, -0.0533,\n",
       "          0.0228,  0.2413,  0.1846, -0.0804,  0.0341, -0.0054, -0.2438,  0.1163]],\n",
       "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7ff64ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-a293cadf22b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multilabel-indicator\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: multiclass-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "y_pred = [(0, 2),( 1, 3)]\n",
    "y_true = [(0, 1), (2, 3)]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf293bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
