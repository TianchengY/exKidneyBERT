{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee773047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead,BertForSequenceClassification, AutoTokenizer,AutoModelForQuestionAnswering, AutoModel,AutoModelForMaskedLM,AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score,roc_curve\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AdamW,get_scheduler\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4567df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcdc26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_datasets(train_text_0,train_text,test_text_0,test_text,tokenizer=tokenizer):\n",
    "\n",
    "    train_encodings = tokenizer(train_text_0,train_text,padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512,return_offsets_mapping=True)\n",
    "    test_encodings = tokenizer(test_text_0,test_text,padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512,return_offsets_mapping=True)\n",
    "    train_dataset = QADataset(train_encodings, train_labels)\n",
    "    test_dataset = QADataset(test_encodings, test_labels)\n",
    "    return train_dataset,test_dataset\n",
    "\n",
    "\n",
    "\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels,task_name=None):\n",
    "        self.encodings = encodings\n",
    "        self.answers = labels\n",
    "        self.task_name = task_name\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        answer = self.answers[idx]\n",
    "        offsets = inputs.pop(\"offset_mapping\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        cls_index = list(input_ids).index(tokenizer.cls_token_id)\n",
    "\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "#         print(\"Asd\",answer)\n",
    "\n",
    "        if answer[1] == 0:\n",
    "            inputs[\"start_positions\"] = cls_index\n",
    "            inputs[\"end_positions\"] = cls_index\n",
    "        else:\n",
    "            start_char = answer[0]\n",
    "            end_char = answer[1]\n",
    "\n",
    "            token_start_index = 0\n",
    "            while token_type_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while offsets[token_end_index][1] == 0:\n",
    "                token_end_index -= 1\n",
    "                \n",
    "#             print(offsets[token_start_index][0] , start_char,answer)\n",
    "\n",
    "#             print(token_start_index,token_end_index)\n",
    "#             print(offsets[token_start_index], offsets[token_end_index])\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                inputs[\"start_positions\"] = cls_index\n",
    "                inputs[\"end_positions\"] = cls_index\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "#                     print(offsets[token_start_index],token_start_index)\n",
    "                    token_start_index += 1\n",
    "                inputs[\"start_positions\"] = token_start_index - 1\n",
    "\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                inputs[\"end_positions\"] = token_end_index + 1\n",
    "        inputs[\"start_positions\"] = torch.tensor(inputs[\"start_positions\"])\n",
    "        inputs[\"end_positions\"] = torch.tensor(inputs[\"end_positions\"])\n",
    "#         inputs[\"labels\"] = (inputs[\"start_positions\"],inputs[\"end_positions\"])\n",
    "#         print(inputs[\"start_positions\"],inputs[\"end_positions\"])\n",
    "        return inputs\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d0c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def get_overlap_ratio(s1, s2):\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    pos_a, pos_b, size = s.find_longest_match(0, len(s1), 0, len(s2)) \n",
    "#     print(s1,s2,s1[pos_a:pos_a+size])\n",
    "    return size/len(s2),len(s1[pos_a:pos_a+size].split())/len(s2.split())\n",
    "\n",
    "def compute_metrics(p):   \n",
    "    \n",
    "    pred, labels = p   \n",
    "        \n",
    "    answer_start_scores, answer_end_scores = pred\n",
    "    answer_start = np.argmax(answer_start_scores, axis=1)  # get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = np.argmax(answer_end_scores, axis=1)+1\n",
    "    \n",
    "    \n",
    "    total = 0\n",
    "    correct,correct_with_info = 0,0\n",
    "    overlap_ratio_char = []\n",
    "    overlap_ratio_word = []\n",
    "    for s,e,t,id in zip(answer_start,answer_end,test_ans,test_ids):\n",
    "        total += 1\n",
    "        pred_ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(id[s:e]))\n",
    "#         print(\"qweretw\",pred_ans,t,s,e)\n",
    "        if s == 0 and e == 1 and t == \"\":\n",
    "            correct += 1\n",
    "        elif not (s == 0 and e == 1) and t!=\"\":\n",
    "            if pred_ans.lower().replace('\\n', ' ')==t.lower():\n",
    "                correct_with_info += 1\n",
    "            char_ratio,word_ratio = get_overlap_ratio(pred_ans.lower().replace('\\n', ' '),t.lower())\n",
    "            overlap_ratio_char.append(char_ratio)\n",
    "            overlap_ratio_word.append(word_ratio)\n",
    "    \n",
    "    return {\"accuracy\": (correct+correct_with_info)/total,\"accuracy_info\": correct_with_info/total,\\\n",
    "            \"overlap_ratio_char\":np.mean(overlap_ratio_char),\"overlap_ratio_word\":np.mean(overlap_ratio_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474c6214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\ytcsc\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3dad87a61a4bf3b1e4bc4480fc7879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad_raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a43202",
   "metadata": {},
   "outputs": [],
   "source": [
    "text,label = [],[]\n",
    "for example in squad_raw_datasets[\"train\"]:\n",
    "    text.append((example[\"question\"],example[\"context\"]))\n",
    "    label.append((example[\"answers\"][\"answer_start\"][0],(example[\"answers\"][\"answer_start\"][0]+len(example[\"answers\"][\"text\"][0]))))\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a87567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(\n",
    "    text, label,random_state = 1,test_size=0.05)\n",
    "\n",
    "\n",
    "train_text_0,train_text_1 = [i[0] for i in train_text],[i[1] for i in train_text]\n",
    "test_text_0,test_text_1 = [i[0] for i in test_text],[i[1] for i in test_text]\n",
    "\n",
    "train_dataset,test_dataset = gen_datasets(train_text_0,train_text_1,test_text_0,test_text_1)\n",
    "squad_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "squad_test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = batch_size)\n",
    "\n",
    "test_ans = []\n",
    "for i,l in zip(test_text_1,test_labels):\n",
    "    test_ans.append(i[l[0]:l[1]])\n",
    "\n",
    "test_ids = torch.tensor([])\n",
    "for i in squad_test_loader:\n",
    "    test_ids = torch.cat((test_ids,i[\"input_ids\"]),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87df41a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./mlm_results_largeData/checkpoint-1100 were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./mlm_results_largeData/checkpoint-1100 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ytcsc\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 83219\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 78030\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16037' max='78030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16037/78030 1:31:25 < 5:53:29, 2.92 it/s, Epoch 3.08/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.339900</td>\n",
       "      <td>1.502680</td>\n",
       "      <td>0.395434</td>\n",
       "      <td>0.395434</td>\n",
       "      <td>0.692150</td>\n",
       "      <td>0.769941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.524300</td>\n",
       "      <td>1.282676</td>\n",
       "      <td>0.445890</td>\n",
       "      <td>0.445890</td>\n",
       "      <td>0.741760</td>\n",
       "      <td>0.814256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.352500</td>\n",
       "      <td>1.195591</td>\n",
       "      <td>0.473973</td>\n",
       "      <td>0.473973</td>\n",
       "      <td>0.754085</td>\n",
       "      <td>0.819647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.263200</td>\n",
       "      <td>1.122702</td>\n",
       "      <td>0.489498</td>\n",
       "      <td>0.489498</td>\n",
       "      <td>0.761616</td>\n",
       "      <td>0.823070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.223900</td>\n",
       "      <td>1.099635</td>\n",
       "      <td>0.500457</td>\n",
       "      <td>0.500457</td>\n",
       "      <td>0.782810</td>\n",
       "      <td>0.847129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.006300</td>\n",
       "      <td>1.156410</td>\n",
       "      <td>0.501598</td>\n",
       "      <td>0.501598</td>\n",
       "      <td>0.782481</td>\n",
       "      <td>0.845786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.953500</td>\n",
       "      <td>1.086520</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.789899</td>\n",
       "      <td>0.851802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.938400</td>\n",
       "      <td>1.085947</td>\n",
       "      <td>0.511187</td>\n",
       "      <td>0.511187</td>\n",
       "      <td>0.786100</td>\n",
       "      <td>0.848096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.956600</td>\n",
       "      <td>1.088878</td>\n",
       "      <td>0.521233</td>\n",
       "      <td>0.521233</td>\n",
       "      <td>0.781837</td>\n",
       "      <td>0.844475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.914600</td>\n",
       "      <td>1.037345</td>\n",
       "      <td>0.520776</td>\n",
       "      <td>0.520776</td>\n",
       "      <td>0.790638</td>\n",
       "      <td>0.851190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.777700</td>\n",
       "      <td>1.146681</td>\n",
       "      <td>0.514155</td>\n",
       "      <td>0.514155</td>\n",
       "      <td>0.794566</td>\n",
       "      <td>0.858192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>1.121879</td>\n",
       "      <td>0.518721</td>\n",
       "      <td>0.518721</td>\n",
       "      <td>0.796225</td>\n",
       "      <td>0.858963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>1.119368</td>\n",
       "      <td>0.523059</td>\n",
       "      <td>0.523059</td>\n",
       "      <td>0.789400</td>\n",
       "      <td>0.852646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>1.064272</td>\n",
       "      <td>0.529452</td>\n",
       "      <td>0.529452</td>\n",
       "      <td>0.793781</td>\n",
       "      <td>0.857137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.703400</td>\n",
       "      <td>1.095137</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.798366</td>\n",
       "      <td>0.860074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.621300</td>\n",
       "      <td>1.278624</td>\n",
       "      <td>0.519863</td>\n",
       "      <td>0.519863</td>\n",
       "      <td>0.790718</td>\n",
       "      <td>0.853592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-1000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-2000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-3000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-4000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-4000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-5000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-5000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-6000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-6000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-7000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-7000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-8000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-8000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-9000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-9000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-10000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-10000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-11000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-11000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-12000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-12000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-13000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-14000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-15000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-15000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4380\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_squad_fine_kidneybert\\checkpoint-16000\n",
      "Configuration saved in ./qa_squad_fine_kidneybert\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./qa_squad_fine_kidneybert\\checkpoint-16000\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9352/1037371390.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./mlm_results_largeData/checkpoint-1100\")\n",
    "\n",
    "steps = 1000\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qa_squad_fine_kidneybert',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_kidneyBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb04d37",
   "metadata": {},
   "source": [
    "## Fine-tune on Bioasq dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "225e20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"training10b.json\")\n",
    "bioasq_raw_datasets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa2481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_lst = []\n",
    "text_lst = []\n",
    "for cate in bioasq_raw_datasets[\"questions\"]:\n",
    "    if cate[\"type\"] == \"factoid\":\n",
    "        text = []\n",
    "        exact_answer = cate[\"exact_answer\"][0]\n",
    "        for t in cate[\"snippets\"]:\n",
    "            text.append(t[\"text\"])\n",
    "        whole_text = \" \".join(text)\n",
    "        ans = whole_text.find(exact_answer)\n",
    "        ans = (ans,ans+len(exact_answer)) if ans!=-1 else (0,0)\n",
    "        ans_lst.append(ans)\n",
    "        text_lst.append((cate[\"body\"],whole_text))\n",
    "        \n",
    "text,label = text_lst,ans_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16016bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(\n",
    "    text, label,random_state = 1,test_size=0.05)\n",
    "\n",
    "\n",
    "train_text_0,train_text_1 = [i[0] for i in train_text],[i[1] for i in train_text]\n",
    "test_text_0,test_text_1 = [i[0] for i in test_text],[i[1] for i in test_text]\n",
    "\n",
    "train_dataset,test_dataset = gen_datasets(train_text_0,train_text_1,test_text_0,test_text_1)\n",
    "bioasq_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "bioasq_test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = batch_size)\n",
    "\n",
    "test_ans = []\n",
    "for i,l in zip(test_text_1,test_labels):\n",
    "    test_ans.append(i[l[0]:l[1]])\n",
    "\n",
    "test_ids = torch.tensor([])\n",
    "for i in bioasq_test_loader:\n",
    "    test_ids = torch.cat((test_ids,i[\"input_ids\"]),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af412c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ytcsc\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1189\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/1125 02:53 < 03:36, 2.88 it/s, Epoch 6.67/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Info</th>\n",
       "      <th>Overlap Ratio Char</th>\n",
       "      <th>Overlap Ratio Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.557900</td>\n",
       "      <td>2.229324</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.683629</td>\n",
       "      <td>0.784050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.882500</td>\n",
       "      <td>2.093555</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.758989</td>\n",
       "      <td>0.836111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.598800</td>\n",
       "      <td>2.035891</td>\n",
       "      <td>0.396825</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.736598</td>\n",
       "      <td>0.864943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.074300</td>\n",
       "      <td>2.436898</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.733175</td>\n",
       "      <td>0.821839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.713300</td>\n",
       "      <td>2.835022</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.756607</td>\n",
       "      <td>0.806424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.518700</td>\n",
       "      <td>2.910207</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.758250</td>\n",
       "      <td>0.812222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>3.518832</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.750808</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.218900</td>\n",
       "      <td>3.646577</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.837382</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>4.109746</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.761455</td>\n",
       "      <td>0.851496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>4.268016</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.762434</td>\n",
       "      <td>0.864734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-50\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-50\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-50\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-100\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-100\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-150\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-150\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-150\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-200\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-200\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-250\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-250\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-250\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-300\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-300\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-350\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-350\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-350\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-400\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-400\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-450\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-450\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-450\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 63\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./qa_bioasq_fine_kidneybert\\checkpoint-500\n",
      "Configuration saved in ./qa_bioasq_fine_kidneybert\\checkpoint-500\\config.json\n",
      "Model weights saved in ./qa_bioasq_fine_kidneybert\\checkpoint-500\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\caffe2\\serialize\\inline_container.cc:300] . unexpected pos 812313664 vs 812313552",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5664/2512933427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1438\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1440\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1441\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1442\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1569\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1570\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   1665\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m             \u001b[1;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1667\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1668\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSCHEDULER_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\caffe2\\serialize\\inline_container.cc:300] . unexpected pos 812313664 vs 812313552"
     ]
    }
   ],
   "source": [
    "qa_squad_kidneyBert = AutoModelForQuestionAnswering.from_pretrained(\"./qa_squad_fine_kidneybert/checkpoint-14000\")\n",
    "\n",
    "steps = 50\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qa_bioasq_fine_kidneybert',          \n",
    "    num_train_epochs=15,              \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    warmup_steps=50,                \n",
    "    weight_decay=1e-2,                          \n",
    "    logging_steps=steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=steps,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps = steps,\n",
    "    save_total_limit = 30,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_squad_kidneyBert,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5f7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81853341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175aa77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18453e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5f5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RejClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RejClassifier, self).__init__()\n",
    "        \n",
    "#         self.base_model = base_model\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.linear = nn.Linear(768, 256) # output features from bert is 768 and 2 is ur number of labels\n",
    "        self.linear2 = nn.Linear(256, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, outputs):\n",
    "        #input_ids, attention_mask = inputs[\"input_ids\"],inputs[\"attention_mask\"]\n",
    "        #print(\"a\")\n",
    "        #outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "#         del inputs[\"labels\"]\n",
    "#         outputs = self.base_model(**inputs)\n",
    "        #print(\"b\")\n",
    "        \n",
    "        #print(\"c\",outputs,outputs.shape)\n",
    "        outputs = self.linear(outputs[1])\n",
    "        outputs = self.dropout(self.relu(outputs))\n",
    "        outputs = self.linear2(outputs)\n",
    "        #print(\"d\",outputs,outputs.shape)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "class IFTAClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IFTAClassifier, self).__init__()\n",
    "        \n",
    "#         self.base_model = base_model\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.linear = nn.Linear(768, 256) # output features from bert is 768 and 2 is ur number of labels\n",
    "        self.linear2 = nn.Linear(256, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, outputs):\n",
    "        #input_ids, attention_mask = inputs[\"input_ids\"],inputs[\"attention_mask\"]\n",
    "#         del inputs[\"labels\"]\n",
    "#         outputs = self.base_model(**inputs)\n",
    "        #print(\"e\",outputs,outputs.shape)\n",
    "        #outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "#         outputs = self.dropout(outputs[1])\n",
    "        #print(\"f\",outputs,outputs.shape)\n",
    "#         outputs = self.linear(outputs)\n",
    "        #print(\"g\",outputs,outputs.shape)\n",
    "        outputs = self.linear(outputs[1])\n",
    "        outputs = self.dropout(self.relu(outputs))\n",
    "        outputs = self.linear2(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d062024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels,task_name=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.task_name = task_name\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        #item[\"task_name\"] = self.task_name\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    #pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred,average=\"micro\")\n",
    "    precision = precision_score(y_true=labels, y_pred=pred,average=\"micro\")\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred,average=\"micro\")\n",
    "    print(\"accuracy: {}, precision: {}, recall: {}, f1: {}\".format(accuracy,precision,recall,f1))\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e3f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_datasets(q,train_text,test_text,tokenizer=tokenizer):\n",
    "    train_q = [q for i in range(len(train_text))]\n",
    "    test_q = [q for i in range(len(test_text))]\n",
    "\n",
    "    train_encodings = tokenizer(train_q,train_text,padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512)\n",
    "    test_encodings = tokenizer(test_q,test_text,padding=\"max_length\", truncation=True, \n",
    "                                return_tensors=\"pt\",max_length=512)\n",
    "    train_dataset = RenalDataset(train_encodings, train_labels)\n",
    "    test_dataset = RenalDataset(test_encodings, test_labels)\n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0abae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483ea51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "inputs1 = data[\"train_rej\"].tolist()\n",
    "label1 = data[\"isRejection\"].tolist()\n",
    "label = [l for i,l in zip(inputs1,label1) if str(i)!=\"nan\"]\n",
    "inputs = [i for i in inputs1 if str(i)!=\"nan\"]\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(\n",
    "    inputs, label,random_state = 1,stratify=label,test_size=0.2)\n",
    "# train_encodings = tokenizer(train_text,padding=\"max_length\", truncation=True, \n",
    "#                             return_tensors=\"pt\",max_length=512)\n",
    "# test_encodings = tokenizer(test_text,padding=\"max_length\", truncation=True, \n",
    "#                             return_tensors=\"pt\",max_length=512)\n",
    "# train_dataset = RenalDataset(train_encodings, train_labels,task_name=\"isrej\")\n",
    "# test_dataset = RenalDataset(test_encodings, test_labels,task_name=\"isrej\")\n",
    "q_rej = \"Is there any rejection?\"\n",
    "train_dataset,test_dataset = gen_datasets(q_rej,train_text,test_text)\n",
    "isrej_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size)\n",
    "isrej_test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "092494d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "inputs1 = data[\"train_ifta\"].tolist()\n",
    "label1 = data[\"IFTA\"].tolist()\n",
    "label2 = [l for i,l in zip(inputs1,label1) if str(i)!=\"nan\"]\n",
    "label = [0 if l in [\"nosig\",\"minimal\",\"noinfo\"] else (1 if l==\"mild\" else (2 if l==\"moderate\" else 3)) for l in label2]\n",
    "inputs = [i for i in inputs1 if str(i)!=\"nan\"]\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(\n",
    "    inputs, label,random_state = 1,stratify=label,test_size=0.2)\n",
    "# train_encodings = tokenizer(train_text,padding=\"max_length\", truncation=True, \n",
    "#                             return_tensors=\"pt\",max_length=512)\n",
    "# test_encodings = tokenizer(test_text,padding=\"max_length\", truncation=True, \n",
    "#                             return_tensors=\"pt\",max_length=512)\n",
    "# train_dataset = RenalDataset(train_encodings, train_labels,task_name = \"ifta\")\n",
    "# test_dataset = RenalDataset(test_encodings, test_labels,task_name = \"ifta\")\n",
    "q_ifta = \"What is the grade of interstitial fibrosis and tubular atrophy?\"\n",
    "train_dataset,test_dataset = gen_datasets(q_ifta,train_text,test_text)\n",
    "ifta_train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = batch_size)\n",
    "ifta_test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38aebd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_isrej = RejClassifier()\n",
    "model_ifta = IFTAClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e32da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ytc19\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer_isrej = AdamW(list(base_kidneyBert.parameters())+list(model_isrej.parameters()), lr=5e-5)\n",
    "optimizer_ifta = AdamW(list(base_kidneyBert.parameters())+list(model_ifta.parameters()), lr=5e-5)\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * min(len(isrej_train_loader),len(ifta_train_loader))\n",
    "lr_scheduler_isrej = get_scheduler(\"linear\", optimizer=optimizer_isrej, num_warmup_steps=30, num_training_steps=num_training_steps)\n",
    "lr_scheduler_ifta = get_scheduler(\"linear\", optimizer=optimizer_ifta, num_warmup_steps=30, num_training_steps=num_training_steps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a6b346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IFTAClassifier(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model_isrej.to(device)\n",
    "model_ifta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa17cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_renal = AutoModelForSequenceClassification.from_pretrained(\"./mlm_results_largeData/checkpoint-1100\",num_labels=4)\n",
    "# model_renal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6a2f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model,dataloader):\n",
    "    pred,labels = [],[]\n",
    "    model.eval() \n",
    "    base_kidneyBert.eval()\n",
    "    with torch.no_grad(): \n",
    "        for batch in dataloader:\n",
    "           \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            cur_labels = batch[\"labels\"].to(\"cpu\").flatten().tolist()\n",
    "            del batch[\"labels\"]\n",
    "            outputs = model(base_kidneyBert(**batch))\n",
    "            predicted = torch.argmax(outputs, axis=1).to(\"cpu\").flatten().tolist() #torch.max(outputs.data,1) \n",
    "            pred+=predicted\n",
    "            labels+=cur_labels\n",
    "            \n",
    "            \n",
    "#             if pred == 0:\n",
    "#                 pred = predicted\n",
    "#                 labels = cur_labels\n",
    "#             else:\n",
    "#                 pred = torch.cat((pred,predicted),-1)\n",
    "#                 labels = torch.cat((labels,cur_labels),-1)\n",
    "#             total += labels.size(0)\n",
    "            \n",
    "#             correct += (predicted == labels).sum().item()\n",
    "            \n",
    "#     print('the accuracy is {:.4f}'.format(correct/total))\n",
    "    return pred,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42705250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54347a29f926447e8d7e941b798e0b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:300, EPOCHS : 2/20 Loss : 0.1552,0.5154\n",
      "accuracy: 0.9445255474452555, precision: 0.9445255474452555, recall: 0.9445255474452555, f1: 0.9445255474452555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.710948905109489, precision: 0.710948905109489, recall: 0.710948905109489, f1: 0.7109489051094892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:600, EPOCHS : 4/20 Loss : 0.1199,0.9246\n",
      "accuracy: 0.962043795620438, precision: 0.962043795620438, recall: 0.962043795620438, f1: 0.962043795620438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7445255474452555, precision: 0.7445255474452555, recall: 0.7445255474452555, f1: 0.7445255474452555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:900, EPOCHS : 5/20 Loss : 0.0424,0.3793\n",
      "accuracy: 0.9518248175182482, precision: 0.9518248175182482, recall: 0.9518248175182482, f1: 0.9518248175182482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.710948905109489, precision: 0.710948905109489, recall: 0.710948905109489, f1: 0.7109489051094892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1200, EPOCHS : 7/20 Loss : 0.0124,0.5633\n",
      "accuracy: 0.9635036496350365, precision: 0.9635036496350365, recall: 0.9635036496350365, f1: 0.9635036496350365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7489051094890511, precision: 0.7489051094890511, recall: 0.7489051094890511, f1: 0.7489051094890511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1500, EPOCHS : 8/20 Loss : 0.0013,0.4945\n",
      "accuracy: 0.9576642335766423, precision: 0.9576642335766423, recall: 0.9576642335766423, f1: 0.9576642335766423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7445255474452555, precision: 0.7445255474452555, recall: 0.7445255474452555, f1: 0.7445255474452555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1800, EPOCHS : 10/20 Loss : 0.0488,0.4588\n",
      "accuracy: 0.9445255474452555, precision: 0.9445255474452555, recall: 0.9445255474452555, f1: 0.9445255474452555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.708029197080292, precision: 0.708029197080292, recall: 0.708029197080292, f1: 0.708029197080292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2100, EPOCHS : 11/20 Loss : 0.0332,0.3511\n",
      "accuracy: 0.9605839416058394, precision: 0.9605839416058394, recall: 0.9605839416058394, f1: 0.9605839416058394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7635036496350365, precision: 0.7635036496350365, recall: 0.7635036496350365, f1: 0.7635036496350364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2400, EPOCHS : 13/20 Loss : 0.0017,0.3229\n",
      "accuracy: 0.9547445255474453, precision: 0.9547445255474453, recall: 0.9547445255474453, f1: 0.9547445255474453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7547445255474453, precision: 0.7547445255474453, recall: 0.7547445255474453, f1: 0.7547445255474453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2700, EPOCHS : 14/20 Loss : 0.0013,0.1532\n",
      "accuracy: 0.9518248175182482, precision: 0.9518248175182482, recall: 0.9518248175182482, f1: 0.9518248175182482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7211678832116788, precision: 0.7211678832116788, recall: 0.7211678832116788, f1: 0.7211678832116789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3000, EPOCHS : 16/20 Loss : 0.0007,0.4339\n",
      "accuracy: 0.9518248175182482, precision: 0.9518248175182482, recall: 0.9518248175182482, f1: 0.9518248175182482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7635036496350365, precision: 0.7635036496350365, recall: 0.7635036496350365, f1: 0.7635036496350364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3300, EPOCHS : 17/20 Loss : 0.0012,0.0808\n",
      "accuracy: 0.945985401459854, precision: 0.945985401459854, recall: 0.945985401459854, f1: 0.945985401459854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7532846715328467, precision: 0.7532846715328467, recall: 0.7532846715328467, f1: 0.7532846715328468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3600, EPOCHS : 19/20 Loss : 0.0002,0.0347\n",
      "accuracy: 0.9576642335766423, precision: 0.9576642335766423, recall: 0.9576642335766423, f1: 0.9576642335766423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7766423357664234, precision: 0.7766423357664234, recall: 0.7766423357664234, f1: 0.7766423357664234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3900, EPOCHS : 20/20 Loss : 0.0002,0.0380\n",
      "accuracy: 0.9562043795620438, precision: 0.9562043795620438, recall: 0.9562043795620438, f1: 0.9562043795620438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7737226277372263, precision: 0.7737226277372263, recall: 0.7737226277372263, f1: 0.7737226277372263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "best_f1_isrej = 0\n",
    "best_f1_ifta = 0\n",
    "step = 0\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_isrej,batch_ifta in zip(isrej_train_loader,ifta_train_loader):\n",
    "            model_isrej.train()\n",
    "            model_ifta.train()\n",
    "            base_kidneyBert.train()\n",
    "            step+=1\n",
    "            #print(step)\n",
    "            #print(1)\n",
    "            \n",
    "            \n",
    "            batch_isrej = {k: v.to(device) for k, v in batch_isrej.items()}\n",
    "            labels = batch_isrej[\"labels\"]\n",
    "#             labels = copy.deepcopy(batch_isrej[\"labels\"]).to(device)\n",
    "            #print(2)\n",
    "            del batch_isrej[\"labels\"]\n",
    "            feat = base_kidneyBert(**batch_isrej)\n",
    "#             print(feat[0].shape,feat[1].shape)\n",
    "            outputs = model_isrej(feat)\n",
    "            #print(3)\n",
    "            #print(4)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "            #print(outputs,outputs.shape)\n",
    "            #print(labels,labels.shape)\n",
    "            loss_isrej = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "            #print(6)\n",
    "            loss_isrej.backward()\n",
    "            #print(7)\n",
    "\n",
    "            optimizer_isrej.step()\n",
    "            lr_scheduler_isrej.step()\n",
    "            optimizer_isrej.zero_grad()\n",
    "#             for k, v in batch_isrej.items():\n",
    "#                 v.detach()\n",
    "# #                 del v\n",
    "#             batch_isrej.clear()\n",
    "                \n",
    "#             batch_isrej = 0\n",
    "            \n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "\n",
    "            batch_ifta = {k: v.to(device) for k, v in batch_ifta.items()}\n",
    "            labels = batch_ifta[\"labels\"]\n",
    "            del batch_ifta[\"labels\"]\n",
    "            feat = base_kidneyBert(**batch_ifta)\n",
    "            outputs = model_ifta(feat)\n",
    "#             outputs = model_ifta(batch_ifta)\n",
    "            #print(10)\n",
    "            \n",
    "            #print(11)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "            #print(12)\n",
    "            #print(\"q\",outputs,outputs.shape)\n",
    "            #print(\"z\",labels,labels.shape)\n",
    "            loss_ifta = loss_fct(outputs.view(-1, 4), labels.view(-1))\n",
    "            loss_ifta.backward()\n",
    "\n",
    "            optimizer_ifta.step()\n",
    "            lr_scheduler_ifta.step()\n",
    "            optimizer_ifta.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "            if step % 300 == 0:\n",
    "                #test the accuracy\n",
    "\n",
    "                print('STEP:{}, EPOCHS : {}/{}'.format(step,epoch+1,num_epochs),\n",
    "                      'Loss : {:.4f},{:.4f}'.format(loss_isrej,loss_ifta))\n",
    "                \n",
    "                res_isrej = compute_metrics(get_pred(model_isrej,isrej_test_loader))\n",
    "                res_ifta = compute_metrics(get_pred(model_ifta,ifta_test_loader))\n",
    "                if res_isrej[\"f1\"] > best_f1_isrej and res_ifta[\"f1\"] > best_f1_ifta:\n",
    "                    best_f1_isrej = res_isrej[\"f1\"]\n",
    "                    best_f1_ifta = res_ifta[\"f1\"]\n",
    "                    base_kidneyBert.save_pretrained(f\"./fine_both/f1_{best_f1_isrej}_{best_f1_ifta}\")\n",
    "                    torch.save(model_isrej_.state_dict(),f\"./fine_both/model_isrej_{best_f1_isrej}.pth\")\n",
    "                    torch.save(model_ifta.state_dict(),f\"./fine_both/model_ifta_{best_f1_ifta}.pth\")\n",
    "#                 compute_metrics(get_pred(model_ifta,ifta_test_loader))\n",
    "except Exception as e:\n",
    "    print(\"Exception\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4994284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_kidneyBert.save_pretrained(f\"./fine_both/final\")\n",
    "torch.save(model_isrej.state_dict(),f\"./fine_both/model_isrej_final.pth\")\n",
    "torch.save(model_ifta.state_dict(),f\"./fine_both/model_ifta_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6675ac98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_isrej.load_state_dict(torch.load(\"./fine_both/model_isrej_final.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b26ff0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[638   9]\n",
      " [ 21  17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       647\n",
      "           1       0.65      0.45      0.53        38\n",
      "\n",
      "    accuracy                           0.96       685\n",
      "   macro avg       0.81      0.72      0.75       685\n",
      "weighted avg       0.95      0.96      0.95       685\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[324  43   0   0]\n",
      " [ 37 164  23   0]\n",
      " [  3  31  38   0]\n",
      " [  3   4  12   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       367\n",
      "           1       0.68      0.73      0.70       224\n",
      "           2       0.52      0.53      0.52        72\n",
      "           3       1.00      0.14      0.24        22\n",
      "\n",
      "    accuracy                           0.77       685\n",
      "   macro avg       0.77      0.57      0.59       685\n",
      "weighted avg       0.78      0.77      0.77       685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels,test_labels = get_pred(model_isrej,isrej_test_loader)\n",
    "print(confusion_matrix(test_labels,pred_labels))\n",
    "print(classification_report(test_labels,pred_labels))\n",
    "pred_labels,test_labels = get_pred(model_ifta,ifta_test_loader)\n",
    "print(confusion_matrix(test_labels,pred_labels))\n",
    "print(classification_report(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac83b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0d3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d4151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "975d05f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1471b661b1a24328bc702210a5ceed2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:300, EPOCHS : 2/20 Loss : 0.2560,0.0000\n",
      "accuracy: 0.9445255474452555, precision: 0.9445255474452555, recall: 0.9445255474452555, f1: 0.9445255474452555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:600, EPOCHS : 4/20 Loss : 0.2050,0.0000\n",
      "accuracy: 0.9562043795620438, precision: 0.9562043795620438, recall: 0.9562043795620438, f1: 0.9562043795620438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:900, EPOCHS : 6/20 Loss : 0.0197,0.0000\n",
      "accuracy: 0.9503649635036496, precision: 0.9503649635036496, recall: 0.9503649635036496, f1: 0.9503649635036496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1200, EPOCHS : 7/20 Loss : 0.0123,0.0000\n",
      "accuracy: 0.9635036496350365, precision: 0.9635036496350365, recall: 0.9635036496350365, f1: 0.9635036496350365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1500, EPOCHS : 9/20 Loss : 0.0687,0.0000\n",
      "accuracy: 0.9547445255474453, precision: 0.9547445255474453, recall: 0.9547445255474453, f1: 0.9547445255474453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1800, EPOCHS : 11/20 Loss : 0.0141,0.0000\n",
      "accuracy: 0.9635036496350365, precision: 0.9635036496350365, recall: 0.9635036496350365, f1: 0.9635036496350365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2100, EPOCHS : 13/20 Loss : 0.0038,0.0000\n",
      "accuracy: 0.964963503649635, precision: 0.964963503649635, recall: 0.964963503649635, f1: 0.964963503649635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2400, EPOCHS : 14/20 Loss : 0.0004,0.0000\n",
      "accuracy: 0.9664233576642336, precision: 0.9664233576642336, recall: 0.9664233576642336, f1: 0.9664233576642337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2700, EPOCHS : 16/20 Loss : 0.0024,0.0000\n",
      "accuracy: 0.9664233576642336, precision: 0.9664233576642336, recall: 0.9664233576642336, f1: 0.9664233576642337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3000, EPOCHS : 18/20 Loss : 0.0005,0.0000\n",
      "accuracy: 0.9693430656934306, precision: 0.9693430656934306, recall: 0.9693430656934306, f1: 0.9693430656934306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3300, EPOCHS : 20/20 Loss : 0.0001,0.0000\n",
      "accuracy: 0.9693430656934306, precision: 0.9693430656934306, recall: 0.9693430656934306, f1: 0.9693430656934306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "best_f1 = 0\n",
    "step = 0\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_isrej,batch_ifta in zip(isrej_train_loader,ifta_train_loader):\n",
    "            model_isrej.train()\n",
    "            model_ifta.train()\n",
    "            base_kidneyBert.train()\n",
    "            step+=1\n",
    "            #print(step)\n",
    "            #print(1)\n",
    "            \n",
    "            \n",
    "            batch_isrej = {k: v.to(device) for k, v in batch_isrej.items()}\n",
    "            labels = batch_isrej[\"labels\"]\n",
    "#             labels = copy.deepcopy(batch_isrej[\"labels\"]).to(device)\n",
    "            #print(2)\n",
    "            del batch_isrej[\"labels\"]\n",
    "            feat = base_kidneyBert(**batch_isrej)\n",
    "#             print(feat[0].shape,feat[1].shape)\n",
    "            outputs = model_isrej(feat)\n",
    "            #print(3)\n",
    "            #print(4)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "            #print(outputs,outputs.shape)\n",
    "            #print(labels,labels.shape)\n",
    "            loss_isrej = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "            #print(6)\n",
    "            loss_isrej.backward()\n",
    "            #print(7)\n",
    "\n",
    "            optimizer_isrej.step()\n",
    "            lr_scheduler_isrej.step()\n",
    "            optimizer_isrej.zero_grad()\n",
    "\n",
    "#             batch_ifta = {k: v.to(device) for k, v in batch_ifta.items()}\n",
    "#             labels = batch_ifta[\"labels\"]\n",
    "#             del batch_ifta[\"labels\"]\n",
    "#             feat = base_kidneyBert(**batch_ifta)\n",
    "#             outputs = model_ifta(feat)\n",
    "# #             outputs = model_ifta(batch_ifta)\n",
    "#             #print(10)\n",
    "            \n",
    "#             #print(11)\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "#             #print(12)\n",
    "#             #print(\"q\",outputs,outputs.shape)\n",
    "#             #print(\"z\",labels,labels.shape)\n",
    "#             loss_ifta = loss_fct(outputs.view(-1, 4), labels.view(-1))\n",
    "#             loss_ifta.backward()\n",
    "\n",
    "#             optimizer_ifta.step()\n",
    "#             lr_scheduler_ifta.step()\n",
    "#             optimizer_ifta.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "            loss_ifta = 0\n",
    "            if step % 300 == 0:\n",
    "                #test the accuracy\n",
    "\n",
    "                print('STEP:{}, EPOCHS : {}/{}'.format(step,epoch+1,num_epochs),\n",
    "                      'Loss : {:.4f},{:.4f}'.format(loss_isrej,loss_ifta))\n",
    "\n",
    "                res = compute_metrics(get_pred(model_isrej,isrej_test_loader))\n",
    "                if res[\"f1\"] > best_f1:\n",
    "                    best_f1 = res[\"f1\"]\n",
    "                    base_kidneyBert.save_pretrained(f\"./fine_rej/f1{best_f1}\")\n",
    "                    torch.save(model_isrej.state_dict(),f\"./fine_rej/model_isrej_{best_f1}.pth\")\n",
    "#                 compute_metrics(get_pred(model_ifta,ifta_test_loader))\n",
    "except Exception as e:\n",
    "    print(\"Exception\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa1124e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_kidneyBert.save_pretrained(f\"./fine_rej/final\")\n",
    "torch.save(model_isrej.state_dict(),f\"./fine_rej/model_isrej_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb0464eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[641   6]\n",
      " [ 15  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       647\n",
      "           1       0.79      0.61      0.69        38\n",
      "\n",
      "    accuracy                           0.97       685\n",
      "   macro avg       0.89      0.80      0.84       685\n",
      "weighted avg       0.97      0.97      0.97       685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels,test_labels = get_pred(model_isrej,isrej_test_loader)\n",
    "print(confusion_matrix(test_labels,pred_labels))\n",
    "print(classification_report(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e7a5a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8196cdbf2f46509ec9b4ea1dcdd7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:300, EPOCHS : 2/20 Loss : 0.0000,0.3606\n",
      "accuracy: 0.7051094890510949, precision: 0.7051094890510949, recall: 0.7051094890510949, f1: 0.7051094890510949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:600, EPOCHS : 4/20 Loss : 0.0000,0.4748\n",
      "accuracy: 0.7284671532846715, precision: 0.7284671532846715, recall: 0.7284671532846715, f1: 0.7284671532846715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:900, EPOCHS : 6/20 Loss : 0.0000,0.3987\n",
      "accuracy: 0.7138686131386861, precision: 0.7138686131386861, recall: 0.7138686131386861, f1: 0.7138686131386861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1200, EPOCHS : 7/20 Loss : 0.0000,0.4292\n",
      "accuracy: 0.7386861313868613, precision: 0.7386861313868613, recall: 0.7386861313868613, f1: 0.7386861313868613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1500, EPOCHS : 9/20 Loss : 0.0000,0.1700\n",
      "accuracy: 0.7343065693430657, precision: 0.7343065693430657, recall: 0.7343065693430657, f1: 0.7343065693430656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1800, EPOCHS : 11/20 Loss : 0.0000,0.0980\n",
      "accuracy: 0.7138686131386861, precision: 0.7138686131386861, recall: 0.7138686131386861, f1: 0.7138686131386861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2100, EPOCHS : 13/20 Loss : 0.0000,0.0224\n",
      "accuracy: 0.7343065693430657, precision: 0.7343065693430657, recall: 0.7343065693430657, f1: 0.7343065693430656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2400, EPOCHS : 14/20 Loss : 0.0000,0.0817\n",
      "accuracy: 0.7386861313868613, precision: 0.7386861313868613, recall: 0.7386861313868613, f1: 0.7386861313868613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2700, EPOCHS : 16/20 Loss : 0.0000,0.0653\n",
      "accuracy: 0.7401459854014598, precision: 0.7401459854014598, recall: 0.7401459854014598, f1: 0.7401459854014598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3000, EPOCHS : 18/20 Loss : 0.0000,0.0056\n",
      "accuracy: 0.7416058394160584, precision: 0.7416058394160584, recall: 0.7416058394160584, f1: 0.7416058394160584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3300, EPOCHS : 20/20 Loss : 0.0000,0.0151\n",
      "accuracy: 0.7386861313868613, precision: 0.7386861313868613, recall: 0.7386861313868613, f1: 0.7386861313868613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "best_f1 = 0\n",
    "step = 0\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_isrej,batch_ifta in zip(isrej_train_loader,ifta_train_loader):\n",
    "            model_isrej.train()\n",
    "            model_ifta.train()\n",
    "            base_kidneyBert.train()\n",
    "            step+=1\n",
    "            #print(step)\n",
    "            #print(1)\n",
    "            \n",
    "            \n",
    "#             batch_isrej = {k: v.to(device) for k, v in batch_isrej.items()}\n",
    "#             labels = batch_isrej[\"labels\"]\n",
    "# #             labels = copy.deepcopy(batch_isrej[\"labels\"]).to(device)\n",
    "#             #print(2)\n",
    "#             del batch_isrej[\"labels\"]\n",
    "#             feat = base_kidneyBert(**batch_isrej)\n",
    "# #             print(feat[0].shape,feat[1].shape)\n",
    "#             outputs = model_isrej(feat)\n",
    "#             #print(3)\n",
    "#             #print(4)\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "#             #print(outputs,outputs.shape)\n",
    "#             #print(labels,labels.shape)\n",
    "#             loss_isrej = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "#             #print(6)\n",
    "#             loss_isrej.backward()\n",
    "#             #print(7)\n",
    "\n",
    "#             optimizer_isrej.step()\n",
    "#             lr_scheduler_isrej.step()\n",
    "#             optimizer_isrej.zero_grad()\n",
    "\n",
    "            batch_ifta = {k: v.to(device) for k, v in batch_ifta.items()}\n",
    "            labels = batch_ifta[\"labels\"]\n",
    "            del batch_ifta[\"labels\"]\n",
    "            feat = base_kidneyBert(**batch_ifta)\n",
    "            outputs = model_ifta(feat)\n",
    "#             outputs = model_ifta(batch_ifta)\n",
    "            #print(10)\n",
    "            \n",
    "            #print(11)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "            #print(12)\n",
    "            #print(\"q\",outputs,outputs.shape)\n",
    "            #print(\"z\",labels,labels.shape)\n",
    "            loss_ifta = loss_fct(outputs.view(-1, 4), labels.view(-1))\n",
    "            loss_ifta.backward()\n",
    "\n",
    "            optimizer_ifta.step()\n",
    "            lr_scheduler_ifta.step()\n",
    "            optimizer_ifta.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "            loss_isrej = 0\n",
    "            if step % 300 == 0:\n",
    "                #test the accuracy\n",
    "\n",
    "                print('STEP:{}, EPOCHS : {}/{}'.format(step,epoch+1,num_epochs),\n",
    "                      'Loss : {:.4f},{:.4f}'.format(loss_isrej,loss_ifta))\n",
    "\n",
    "                res = compute_metrics(get_pred(model_ifta,ifta_test_loader))\n",
    "                if res[\"f1\"] > best_f1:\n",
    "                    best_f1 = res[\"f1\"]\n",
    "                    base_kidneyBert.save_pretrained(f\"./fine_ifta/f1{best_f1}\")\n",
    "                    torch.save(model_ifta.state_dict(),f\"./fine_ifta/model_isrej_{best_f1}.pth\")\n",
    "#                 compute_metrics(get_pred(model_ifta,ifta_test_loader))\n",
    "except Exception as e:\n",
    "    print(\"Exception\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ac4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_kidneyBert.save_pretrained(f\"./fine_ifta/final\")\n",
    "torch.save(model_ifta.state_dict(),f\"./fine_ifta/model_ifta_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65dfd34d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[320  42   3   2]\n",
      " [ 49 144  30   1]\n",
      " [  2  29  40   1]\n",
      " [  2   2  14   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       367\n",
      "           1       0.66      0.64      0.65       224\n",
      "           2       0.46      0.56      0.50        72\n",
      "           3       0.50      0.18      0.27        22\n",
      "\n",
      "    accuracy                           0.74       685\n",
      "   macro avg       0.62      0.56      0.57       685\n",
      "weighted avg       0.74      0.74      0.74       685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels,test_labels = get_pred(model_ifta,ifta_test_loader)\n",
    "print(confusion_matrix(test_labels,pred_labels))\n",
    "print(classification_report(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb52a44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 16.00 GiB total capacity; 14.15 GiB already allocated; 0 bytes free; 14.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "best_f1_isrej = 0\n",
    "best_f1_ifta = 0\n",
    "step = 0\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_isrej,batch_ifta in zip(isrej_train_loader,ifta_train_loader):\n",
    "            model_isrej.train()\n",
    "            model_ifta.train()\n",
    "            base_kidneyBert.train()\n",
    "            step+=1\n",
    "            #print(step)\n",
    "            #print(1)\n",
    "            \n",
    "            \n",
    "            batch_isrej = {k: v.to(device) for k, v in batch_isrej.items()}\n",
    "            labels = batch_isrej[\"labels\"]\n",
    "#             labels = copy.deepcopy(batch_isrej[\"labels\"]).to(device)\n",
    "            #print(2)\n",
    "            del batch_isrej[\"labels\"]\n",
    "            feat = base_kidneyBert(**batch_isrej)\n",
    "#             print(feat[0].shape,feat[1].shape)\n",
    "            outputs = model_isrej(feat)\n",
    "            #print(3)\n",
    "            #print(4)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "            #print(outputs,outputs.shape)\n",
    "            #print(labels,labels.shape)\n",
    "            loss_isrej = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "            #print(6)\n",
    "            loss_isrej.backward()\n",
    "            #print(7)\n",
    "\n",
    "            optimizer_isrej.step()\n",
    "            lr_scheduler_isrej.step()\n",
    "            optimizer_isrej.zero_grad()\n",
    "            for k, v in batch_isrej.items():\n",
    "                del v\n",
    "                \n",
    "            batch_isrej = 0\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "\n",
    "            batch_ifta = {k: v.to(device) for k, v in batch_ifta.items()}\n",
    "            labels = batch_ifta[\"labels\"]\n",
    "            del batch_ifta[\"labels\"]\n",
    "            feat = base_kidneyBert(**batch_ifta)\n",
    "            outputs = model_ifta(feat)\n",
    "#             outputs = model_ifta(batch_ifta)\n",
    "            #print(10)\n",
    "            \n",
    "            #print(11)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "            #print(12)\n",
    "            #print(\"q\",outputs,outputs.shape)\n",
    "            #print(\"z\",labels,labels.shape)\n",
    "            loss_ifta = loss_fct(outputs.view(-1, 4), labels.view(-1))\n",
    "            loss_ifta.backward()\n",
    "\n",
    "            optimizer_ifta.step()\n",
    "            lr_scheduler_ifta.step()\n",
    "            optimizer_ifta.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "            if step % 300 == 0:\n",
    "                #test the accuracy\n",
    "\n",
    "                print('STEP:{}, EPOCHS : {}/{}'.format(step,epoch+1,num_epochs),\n",
    "                      'Loss : {:.4f},{:.4f}'.format(loss_isrej,loss_ifta))\n",
    "                \n",
    "                res_isrej = compute_metrics(get_pred(model_isrej,isrej_test_loader))\n",
    "                res_ifta = compute_metrics(get_pred(model_ifta,ifta_test_loader))\n",
    "                if res_isrej[\"f1\"] > best_f1_isrej and res_ifta[\"f1\"] > best_f1_ifta:\n",
    "                    best_f1_isrej = res_isrej[\"f1\"]\n",
    "                    best_f1_ifta = res_ifta[\"f1\"]\n",
    "                    base_kidneyBert.save_pretrained(f\"./fine_both/f1_{best_f1_isrej}_{best_f1_ifta}\")\n",
    "                    torch.save(model_ifta.state_dict(),f\"./fine_both/model_isrej_{best_f1_isrej}.pth\")\n",
    "                    torch.save(model_ifta.state_dict(),f\"./fine_both/model_ifta_{best_f1_ifta}.pth\")\n",
    "#                 compute_metrics(get_pred(model_ifta,ifta_test_loader))\n",
    "except Exception as e:\n",
    "    print(\"Exception\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da9650c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e791b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fe58caabf84b87818e6fc8d4e28d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "step = 0\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_isrej,batch_ifta in zip(isrej_train_loader,ifta_train_loader):\n",
    "            model_isrej.train()\n",
    "            model_ifta.train()\n",
    "            base_kidneyBert.train()\n",
    "            step+=1\n",
    "            #print(step)\n",
    "            #print(1)\n",
    "            \n",
    "            \n",
    "            batch_isrej = {k: v.to(device) for k, v in batch_isrej.items()}\n",
    "            labels = batch_isrej[\"labels\"]\n",
    "#             labels = copy.deepcopy(batch_isrej[\"labels\"]).to(device)\n",
    "            #print(2)\n",
    "            del batch_isrej[\"labels\"]\n",
    "            feat = base_kidneyBert(**batch_isrej)\n",
    "#             print(feat[0].shape,feat[1].shape)\n",
    "            outputs = model_isrej(feat)\n",
    "            #print(3)\n",
    "            #print(4)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "            #print(outputs,outputs.shape)\n",
    "            #print(labels,labels.shape)\n",
    "            loss_isrej = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "            #print(6)\n",
    "            loss_isrej.backward()\n",
    "            #print(7)\n",
    "\n",
    "            optimizer_isrej.step()\n",
    "            #lr_scheduler_isrej.step()\n",
    "            optimizer_isrej.zero_grad()\n",
    "\n",
    "            batch_ifta = {k: v.to(device) for k, v in batch_ifta.items()}\n",
    "            labels = batch_ifta[\"labels\"]\n",
    "            del batch_ifta[\"labels\"]\n",
    "            feat = base_kidneyBert(**batch_ifta)\n",
    "            outputs = model_ifta(feat)\n",
    "#             outputs = model_ifta(batch_ifta)\n",
    "            #print(10)\n",
    "            \n",
    "            #print(11)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "            #print(12)\n",
    "            #print(\"q\",outputs,outputs.shape)\n",
    "            #print(\"z\",labels,labels.shape)\n",
    "            loss_ifta = loss_fct(outputs.view(-1, 4), labels.view(-1))\n",
    "            loss_ifta.backward()\n",
    "\n",
    "            optimizer_ifta.step()\n",
    "            lr_scheduler_ifta.step()\n",
    "            optimizer_ifta.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "            loss_isrej = 0\n",
    "            if step % 300 == 0:\n",
    "                #test the accuracy\n",
    "\n",
    "                print('STEP:{}, EPOCHS : {}/{}'.format(step,epoch+1,num_epochs),\n",
    "                      'Loss : {:.4f},{:.4f}'.format(loss_isrej,loss_ifta))\n",
    "\n",
    "#                 compute_metrics(get_pred(model_isrej,isrej_test_loader))\n",
    "                compute_metrics(get_pred(model_ifta,ifta_test_loader))\n",
    "except Exception as e:\n",
    "    print(\"Exception\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ccf874e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:300, EPOCHS : 1/10 Loss : 0.0000,0.5015\n",
      "accuracy: 0.7167883211678832, precision: 0.7167883211678832, recall: 0.7167883211678832, f1: 0.7167883211678832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:600, EPOCHS : 1/10 Loss : 0.0000,0.6111\n",
      "accuracy: 0.6875912408759124, precision: 0.6875912408759124, recall: 0.6875912408759124, f1: 0.6875912408759124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:900, EPOCHS : 2/10 Loss : 0.0000,1.2040\n",
      "accuracy: 0.7299270072992701, precision: 0.7299270072992701, recall: 0.7299270072992701, f1: 0.72992700729927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1200, EPOCHS : 2/10 Loss : 0.0000,0.4732\n",
      "accuracy: 0.7094890510948905, precision: 0.7094890510948905, recall: 0.7094890510948905, f1: 0.7094890510948906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1500, EPOCHS : 3/10 Loss : 0.0000,0.6230\n",
      "accuracy: 0.7124087591240876, precision: 0.7124087591240876, recall: 0.7124087591240876, f1: 0.7124087591240876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:1800, EPOCHS : 3/10 Loss : 0.0000,0.6581\n",
      "accuracy: 0.7299270072992701, precision: 0.7299270072992701, recall: 0.7299270072992701, f1: 0.72992700729927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2100, EPOCHS : 4/10 Loss : 0.0000,0.9544\n",
      "accuracy: 0.7197080291970803, precision: 0.7197080291970803, recall: 0.7197080291970803, f1: 0.7197080291970803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2400, EPOCHS : 4/10 Loss : 0.0000,0.6450\n",
      "accuracy: 0.6963503649635037, precision: 0.6963503649635037, recall: 0.6963503649635037, f1: 0.6963503649635037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:2700, EPOCHS : 4/10 Loss : 0.0000,0.0922\n",
      "accuracy: 0.7401459854014598, precision: 0.7401459854014598, recall: 0.7401459854014598, f1: 0.7401459854014598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3000, EPOCHS : 5/10 Loss : 0.0000,0.6349\n",
      "accuracy: 0.7445255474452555, precision: 0.7445255474452555, recall: 0.7445255474452555, f1: 0.7445255474452555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3300, EPOCHS : 5/10 Loss : 0.0000,0.6144\n",
      "accuracy: 0.7401459854014598, precision: 0.7401459854014598, recall: 0.7401459854014598, f1: 0.7401459854014598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3600, EPOCHS : 6/10 Loss : 0.0000,0.2996\n",
      "accuracy: 0.7065693430656934, precision: 0.7065693430656934, recall: 0.7065693430656934, f1: 0.7065693430656934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:3900, EPOCHS : 6/10 Loss : 0.0000,0.2006\n",
      "accuracy: 0.7357664233576642, precision: 0.7357664233576642, recall: 0.7357664233576642, f1: 0.7357664233576642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:4200, EPOCHS : 7/10 Loss : 0.0000,0.4186\n",
      "accuracy: 0.7313868613138687, precision: 0.7313868613138687, recall: 0.7313868613138687, f1: 0.7313868613138687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:4500, EPOCHS : 7/10 Loss : 0.0000,0.7996\n",
      "accuracy: 0.7474452554744525, precision: 0.7474452554744525, recall: 0.7474452554744525, f1: 0.7474452554744525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:4800, EPOCHS : 8/10 Loss : 0.0000,0.3397\n",
      "accuracy: 0.7328467153284671, precision: 0.7328467153284671, recall: 0.7328467153284671, f1: 0.7328467153284671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:5100, EPOCHS : 8/10 Loss : 0.0000,0.9969\n",
      "accuracy: 0.7445255474452555, precision: 0.7445255474452555, recall: 0.7445255474452555, f1: 0.7445255474452555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:5400, EPOCHS : 8/10 Loss : 0.0000,0.1258\n",
      "accuracy: 0.7635036496350365, precision: 0.7635036496350365, recall: 0.7635036496350365, f1: 0.7635036496350364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:5700, EPOCHS : 9/10 Loss : 0.0000,0.1847\n",
      "accuracy: 0.7518248175182481, precision: 0.7518248175182481, recall: 0.7518248175182481, f1: 0.7518248175182483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:6000, EPOCHS : 9/10 Loss : 0.0000,0.0526\n",
      "accuracy: 0.743065693430657, precision: 0.743065693430657, recall: 0.743065693430657, f1: 0.743065693430657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:6300, EPOCHS : 10/10 Loss : 0.0000,0.1960\n",
      "accuracy: 0.743065693430657, precision: 0.743065693430657, recall: 0.743065693430657, f1: 0.743065693430657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:6600, EPOCHS : 10/10 Loss : 0.0000,0.0521\n",
      "accuracy: 0.743065693430657, precision: 0.743065693430657, recall: 0.743065693430657, f1: 0.743065693430657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6049037/ytc1997/renal_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "step = 0\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_isrej,batch_ifta in zip(isrej_train_loader,ifta_train_loader):\n",
    "            model_isrej.train()\n",
    "            model_ifta.train()\n",
    "            base_kidneyBert.train()\n",
    "            step+=1\n",
    "            #print(step)\n",
    "            #print(1)\n",
    "            \n",
    "            \n",
    "#             batch_isrej = {k: v.to(device) for k, v in batch_isrej.items()}\n",
    "#             labels = batch_isrej[\"labels\"]\n",
    "# #             labels = copy.deepcopy(batch_isrej[\"labels\"]).to(device)\n",
    "#             #print(2)\n",
    "#             del batch_isrej[\"labels\"]\n",
    "#             feat = base_kidneyBert(**batch_isrej)\n",
    "# #             print(feat[0].shape,feat[1].shape)\n",
    "#             outputs = model_isrej(feat)\n",
    "#             #print(3)\n",
    "#             #print(4)\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "#             #print(outputs,outputs.shape)\n",
    "#             #print(labels,labels.shape)\n",
    "#             loss_isrej = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "#             #print(6)\n",
    "#             loss_isrej.backward()\n",
    "#             #print(7)\n",
    "\n",
    "#             optimizer_isrej.step()\n",
    "#             #lr_scheduler_isrej.step()\n",
    "#             optimizer_isrej.zero_grad()\n",
    "\n",
    "            batch_ifta = {k: v.to(device) for k, v in batch_ifta.items()}\n",
    "            labels = batch_ifta[\"labels\"]\n",
    "            del batch_ifta[\"labels\"]\n",
    "            feat = base_kidneyBert(**batch_ifta)\n",
    "            outputs = model_ifta(feat)\n",
    "#             outputs = model_ifta(batch_ifta)\n",
    "            #print(10)\n",
    "            \n",
    "            #print(11)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().to(device)\n",
    "            #print(12)\n",
    "            #print(\"q\",outputs,outputs.shape)\n",
    "            #print(\"z\",labels,labels.shape)\n",
    "            loss_ifta = loss_fct(outputs.view(-1, 4), labels.view(-1))\n",
    "            loss_ifta.backward()\n",
    "\n",
    "            optimizer_ifta.step()\n",
    "            lr_scheduler_ifta.step()\n",
    "            optimizer_ifta.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "            loss_isrej = 0\n",
    "            if step % 300 == 0:\n",
    "                #test the accuracy\n",
    "\n",
    "                print('STEP:{}, EPOCHS : {}/{}'.format(step,epoch+1,num_epochs),\n",
    "                      'Loss : {:.4f},{:.4f}'.format(loss_isrej,loss_ifta))\n",
    "\n",
    "#                 compute_metrics(get_pred(model_isrej,isrej_test_loader))\n",
    "                compute_metrics(get_pred(model_ifta,ifta_test_loader))\n",
    "except Exception as e:\n",
    "    print(\"Exception\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99073f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce97145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68a3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "825b8313",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "405aa2ac",
   "metadata": {},
   "source": [
    "## isRejction one task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5068630f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[641   6]\n",
      " [ 15  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       647\n",
      "           1       0.79      0.61      0.69        38\n",
      "\n",
      "    accuracy                           0.97       685\n",
      "   macro avg       0.89      0.80      0.84       685\n",
      "weighted avg       0.97      0.97      0.97       685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels,test_labels = get_pred(model_isrej,isrej_test_loader)\n",
    "print(confusion_matrix(test_labels,pred_labels))\n",
    "print(classification_report(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fc4c0",
   "metadata": {},
   "source": [
    "## isRejction multi tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c0387f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[638   9]\n",
      " [ 21  17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       647\n",
      "           1       0.65      0.45      0.53        38\n",
      "\n",
      "    accuracy                           0.96       685\n",
      "   macro avg       0.81      0.72      0.75       685\n",
      "weighted avg       0.95      0.96      0.95       685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels,test_labels = get_pred(model_isrej,isrej_test_loader)\n",
    "print(confusion_matrix(test_labels,pred_labels))\n",
    "print(classification_report(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a131230",
   "metadata": {},
   "source": [
    "## IFTA one task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a9265c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[320  42   3   2]\n",
      " [ 49 144  30   1]\n",
      " [  2  29  40   1]\n",
      " [  2   2  14   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       367\n",
      "           1       0.66      0.64      0.65       224\n",
      "           2       0.46      0.56      0.50        72\n",
      "           3       0.50      0.18      0.27        22\n",
      "\n",
      "    accuracy                           0.74       685\n",
      "   macro avg       0.62      0.56      0.57       685\n",
      "weighted avg       0.74      0.74      0.74       685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels,test_labels = get_pred(model_ifta,ifta_test_loader)\n",
    "print(confusion_matrix(test_labels,pred_labels))\n",
    "print(classification_report(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18010c5",
   "metadata": {},
   "source": [
    "## IFTA multi tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eba45149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-290a08d6a040>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[324  43   0   0]\n",
      " [ 37 164  23   0]\n",
      " [  3  31  38   0]\n",
      " [  3   4  12   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       367\n",
      "           1       0.68      0.73      0.70       224\n",
      "           2       0.52      0.53      0.52        72\n",
      "           3       1.00      0.14      0.24        22\n",
      "\n",
      "    accuracy                           0.77       685\n",
      "   macro avg       0.77      0.57      0.59       685\n",
      "weighted avg       0.78      0.77      0.77       685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels,test_labels = get_pred(model_ifta,ifta_test_loader)\n",
    "print(confusion_matrix(test_labels,pred_labels))\n",
    "print(classification_report(test_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da90be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672fdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf76a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1d1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1405214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
